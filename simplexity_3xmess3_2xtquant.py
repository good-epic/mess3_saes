# -*- coding: utf-8 -*-
"""Simplexity_3xMess3_2xTQuant

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TdFJvobna_lbkUA-I9TbcIuuFw_ZdgGK
"""

#%%

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Installation (skip if already installed)
# %pip -q install --upgrade pip wheel setuptools
# %pip -q install "einops>=0.7.0" "jaxtyping>=0.2.28" "beartype>=0.14" better_abc
# %pip -q install --no-deps "transformer-lens>=2.16.1"
# %pip -q install "git+https://github.com/Astera-org/simplexity.git@MATS_2025_app"
print("âœ… Ready!")

# Cell 2: Setup - Product of tom_quantum and mess3
import os
os.environ["JAX_PLATFORM_NAME"] = "cpu"
import torch
import torch.nn.functional as F
import numpy as np
import jax
import jax.numpy as jnp
from transformer_lens import HookedTransformer, HookedTransformerConfig
from simplexity.generative_processes.builder import build_hidden_markov_model, build_generalized_hidden_markov_model
from simplexity.generative_processes.torch_generator import generate_data_batch

from tqdm import tqdm
from sklearn.decomposition import PCA
import pickle

#%%
# Define the number of processes for each type
n_mess3 = 3  # Example: Create 2 mess3 processes
n_tom_quantum = 2 # Example: Create 2 tom_quantum processes
checkpoint_path = "outputs/checkpoints/multipartite_1"

# Ensure checkpoint_path exists
os.makedirs(checkpoint_path, exist_ok=True)

# Create multiple instances of each process
m3_x = [round(x, 3) for x in np.linspace(0.1, 0.4, n_mess3)]
m3_a = [round(x, 3) for x in np.linspace(0.2, 0.8, n_mess3)]
mess3_processes = [build_hidden_markov_model(f"mess3", x=x, a=a) for x, a in zip(m3_x, m3_a)]
# Figure out what the ranges are for alpha and beta that are acceptable. Found this in
# simplexity/simplexity/generative_processes/transition_matrices.py:
# def tom_quantum(alpha: float, beta: float) -> jax.Array:
#     """Creates a transition matrix for the Tom Quantum Process."""
#     gamma2 = 1 / (4 * (alpha**2 + beta**2))
#     common_diag = 1 / 4
#     middle_diag = (alpha**2 - beta**2) * gamma2
#     off_diag = 2 * alpha * beta * gamma2
#
#     transition_matrices = jnp.array(
#         [
#             [
#                 [common_diag, 0, off_diag],
#                 [0, middle_diag, 0],
#                 [off_diag, 0, common_diag],
#             ],
#             [
#                 [common_diag, 0, -off_diag],
#                 [0, middle_diag, 0],
#                 [-off_diag, 0, common_diag],
#             ],
#             [
#                 [common_diag, off_diag, 0],
#                 [off_diag, common_diag, 0],
#                 [0, 0, middle_diag],
#             ],
#             [
#                 [common_diag, -off_diag, 0],
#                 [-off_diag, common_diag, 0],
#                 [0, 0, middle_diag],
#             ],
#         ]
#     )
#     return transition_matrices
tom_quantum_processes = [build_generalized_hidden_markov_model(f"tom_quantum", alpha=1., beta=np.sqrt(51)) for i in range(n_tom_quantum)]

# Assuming all instances of a type have the same vocab size and states
# Handle the case where n_mess3 or n_tom_quantum is 0
mess3_vocab_size = mess3_processes[0].vocab_size if n_mess3 > 0 else 1
mess3_num_states = mess3_processes[0].num_states if n_mess3 > 0 else 1
tom_quantum_vocab_size = tom_quantum_processes[0].vocab_size if n_tom_quantum > 0 else 1
tom_quantum_num_states = tom_quantum_processes[0].num_states if n_tom_quantum > 0 else 1

#%%
print(tom_quantum_vocab_size)
print(mess3_vocab_size)

#%%

print(f"mess3 processes: {n_mess3} instances with vocab_size={mess3_vocab_size}, states={mess3_num_states}")
print(f"tom_quantum processes: {n_tom_quantum} instances with vocab_size={tom_quantum_vocab_size}, states={tom_quantum_num_states}")

#%%
# Product space has vocab_size =  n_tom_quantum * tom_quantum_vocab_size * n_mess3 * mess3_vocab_size
# This calculation needs to be adjusted based on how the product space is formed.
# Assuming the product space is formed by combining outputs from one instance of each type
# The total vocab size will be the product of the vocab sizes of all individual processes.
# If we are combining one output from *each* mess3 and *each* tom_quantum process:
product_vocab_size = (tom_quantum_vocab_size ** n_tom_quantum) * (mess3_vocab_size ** n_mess3)


print(f"Product space: vocab_size={product_vocab_size}")

# Create TransformerLens model for product space
device = "cuda" if torch.cuda.is_available() else "cpu"
cfg = HookedTransformerConfig(
    d_model=128,  # Bigger model for more complex data
    n_heads=4,
    n_layers=3,
    n_ctx=16,
    d_vocab=product_vocab_size,  # Adjusted vocab size
    act_fn="relu",
    device=device,
    d_head=32
)
model = HookedTransformer(cfg)
print(f"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}")

#%%
# Cell 3: Train on Product Space
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
losses = []
batch_size, seq_len = 2048, cfg.n_ctx
key = jax.random.PRNGKey(42)

# Get stationary distributions for all processes
tom_stationaries = [p.initial_state for p in tom_quantum_processes]
mess3_stationaries = [p.initial_state for p in mess3_processes]

# List to store cumulative variance data and activations
cumulative_variance_data = []
all_layer_activations = {} # Dictionary to store activations for each layer

# Create the tqdm progress bar object
num_steps = 50000
progress_bar = tqdm(range(num_steps), desc="Training", miniters=100)

for step in progress_bar:
    # Generate NEW batch each time
    key, *subkeys = jax.random.split(key, 1 + n_tom_quantum + n_mess3)

    tom_inputs_list = []
    for i in range(n_tom_quantum):
        tom_states = jnp.repeat(tom_stationaries[i][None, :], batch_size, axis=0)
        _, tom_inputs, _ = generate_data_batch(tom_states, tom_quantum_processes[i], batch_size, seq_len, subkeys[i])
        if isinstance(tom_inputs, torch.Tensor):
            tom_inputs_list.append(tom_inputs.cpu().numpy())
        else:
            tom_inputs_list.append(np.array(tom_inputs))

    mess3_inputs_list = []
    for i in range(n_mess3):
        mess3_states = jnp.repeat(mess3_stationaries[i][None, :], batch_size, axis=0)
        _, mess3_inputs, _ = generate_data_batch(mess3_states, mess3_processes[i], batch_size, seq_len, subkeys[n_tom_quantum + i])
        if isinstance(mess3_inputs, torch.Tensor):
            mess3_inputs_list.append(mess3_inputs.cpu().numpy())
        else:
            mess3_inputs_list.append(np.array(mess3_inputs))

    # Combine into product space: token = sum(input_i * base_i)
    # The base for each process will be the product of the vocab sizes of the processes that come after it in the combination.
    # For example, if we have tom1, tom2, mess3_1, mess3_2 with vocab sizes T1, T2, M1, M2
    # token = tom1 * (T2 * M1 * M2) + tom2 * (M1 * M2) + mess3_1 * M2 + mess3_2
    # Assuming all tom_quantum have the same vocab size (tom_quantum_vocab_size) and all mess3 have the same (mess3_vocab_size)
    # token = tom1 * (tom_quantum_vocab_size**(n_tom_quantum-1) * mess3_vocab_size**n_mess3) + ... + mess3_2

    # Initialize product_tokens based on whether there are any inputs
    if n_tom_quantum > 0:
        product_tokens = np.zeros_like(tom_inputs_list[0])
    elif n_mess3 > 0:
        product_tokens = np.zeros_like(mess3_inputs_list[0])
    else:
        # If both are zero, there are no inputs, which might be an edge case
        # Depending on the expected behavior, you might want to skip the training step
        # or handle this differently. For now, let's assume it won't happen or
        # we create a dummy array if needed, though training won't be meaningful.
        # Let's raise an error for clarity if both are zero.
        raise ValueError("Both n_tom_quantum and n_mess3 are zero. Cannot generate data.")


    current_base = product_vocab_size
    # Add tom_quantum contributions
    for i in range(n_tom_quantum):
        # Adjust current_base calculation based on the ordering of combination
        # Assuming tom_quantum processes come before mess3 processes
        base_for_tom = (tom_quantum_vocab_size**(n_tom_quantum - 1 - i)) * (mess3_vocab_size**n_mess3)
        product_tokens += tom_inputs_list[i] * base_for_tom

    # Add mess3 contributions
    # The base for the first mess3 process is the product of vocab sizes of the remaining mess3 processes
    for i in range(n_mess3):
         base_for_mess3 = (mess3_vocab_size**(n_mess3 - 1 - i))
         product_tokens += mess3_inputs_list[i] * base_for_mess3


    tokens = torch.from_numpy(product_tokens).long().to(device)

    # Train step
    loss = model(tokens, return_type="loss")
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    losses.append(loss.item())

    # Save cumulative variance and activations every 100 steps
    if (step + 1) % 5000 == 0:
        # Run with cache to get activations for PCA
        with torch.no_grad(): # No need to track gradients for this
            logits, cache = model.run_with_cache(tokens)

        # Store activations for each layer
        current_step_activations = {}
        for layer in range(cfg.n_layers):
            hook_name = f'blocks.{layer}.hook_resid_post'
            if hook_name in cache:
                current_step_activations[f'layer_{layer}'] = cache[hook_name].cpu().numpy()
            else:
                print(f"Warning: Hook '{hook_name}' not found in cache.")

        # Append activations for this step to the main dictionary
        all_layer_activations[step + 1] = current_step_activations


        # Perform PCA and calculate cumulative variance for the last layer
        if f'layer_{cfg.n_layers - 1}' in current_step_activations:
            last_layer_activations = current_step_activations[f'layer_{cfg.n_layers - 1}']
            # Flatten for PCA
            activations_flat = last_layer_activations.reshape(-1, last_layer_activations.shape[-1])

            # Perform PCA
            pca = PCA() # No n_components here to get all components
            pca.fit(activations_flat)

            # Calculate cumulative explained variance
            cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

            # Store the data
            cumulative_variance_data.append({'step': step + 1, 'cumulative_variance': cumulative_variance})
        else:
             print(f"Warning: Activations for layer {cfg.n_layers - 1} not found in cache for PCA.")

    # Save a checkpoint every 5% of the way through training (but not the final checkpoint here)
    checkpoint_interval = max(1, int(0.05 * num_steps))
    if (step + 1) % checkpoint_interval == 0:
        checkpoint = {
            'step': step + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'losses': losses,
        }
        checkpoint_filename = os.path.join(checkpoint_path, f'checkpoint_step_{step + 1}.pt')
        torch.save(checkpoint, checkpoint_filename)
        print(f"Checkpoint saved at step {step + 1} to {checkpoint_filename}")


    progress_bar.set_description(f"Training (Loss: {loss.item():.4f})")

# Save the final checkpoint after training is complete
final_checkpoint = {
    'step': num_steps,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'losses': losses,
}
final_checkpoint_filename = os.path.join(checkpoint_path, f'checkpoint_step_{num_steps}_final.pt')
torch.save(final_checkpoint, final_checkpoint_filename)
print(f"Final checkpoint saved at step {num_steps} to {final_checkpoint_filename}")

#%%


print(f"\nFinal loss: {losses[-1]:.4f} (started at {losses[0]:.4f})")

# Save the cumulative variance data to a file
with open('cumulative_variance_data.pkl', 'wb') as f:
    pickle.dump(cumulative_variance_data, f)

# Save the layer activations data to a file
with open('all_layer_activations.pkl', 'wb') as f:
    pickle.dump(all_layer_activations, f)

print("Cumulative variance data saved to cumulative_variance_data.pkl")
print("All layer activations data saved to all_layer_activations.pkl")

#%%
# Visualization of loss
import matplotlib.pyplot as plt
plt.plot(losses)
plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training Loss on Product Space (multiple processes)')
plt.show()

# Visualization of loss
import matplotlib.pyplot as plt
import numpy as np

# Calculate moving average
window_size = 50 # You can adjust the window size
moving_average = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')

plt.figure()
plt.plot(losses, alpha=0.5, label='Original Loss') # Plot original loss with transparency
plt.plot(range(window_size - 1, len(losses)), moving_average, label=f'Moving Average (window={window_size})') # Plot moving average

plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training Loss on Product Space (multiple processes)')
plt.ylim([6.045,6.07])
#plt.yscale('log')  # Set y-axis to logarithmic scale
# Add vertical dotted lines
plt.axvline(x=5000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=10000, color='gray', linestyle=':', label='10 dims')
plt.legend()
plt.show()

#%%
import pickle
import matplotlib.pyplot as plt
import numpy as np

# Load the cumulative variance data
try:
    with open('cumulative_variance_data.pkl', 'rb') as f:
        cumulative_variance_data = pickle.load(f)
except FileNotFoundError:
    print("Error: cumulative_variance_data.pkl not found. Please run the training cell first.")
    cumulative_variance_data = []

if cumulative_variance_data:
    steps = []
    dims_to_90_var = []

    for item in cumulative_variance_data:
        step = item['step']
        cumulative_variance = item['cumulative_variance']

        # Find the number of components to reach 90% cumulative variance
        # np.argmax returns the index of the first occurrence of the maximum value.
        # We want the first index where cumulative variance is >= 0.9
        # Adding 1 because indices are 0-based and components are 1-based
        components_needed = np.argmax(cumulative_variance >= 0.95) + 1

        steps.append(step)
        dims_to_90_var.append(components_needed)

    plt.figure(figsize=(10, 6))
    plt.plot(steps, dims_to_90_var, marker='o', linestyle='-')

    plt.xlabel('Training Step')
    plt.ylabel('Number of Components to Explain 90% Variance')
    plt.title('Dimensionality to Explain 90% Variance Over Training Time')
    plt.grid(True)
    plt.show()
else:
    print("No cumulative variance data to plot.")

#%%
import pickle
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.cm as cm

# Load the cumulative variance data
try:
    with open('cumulative_variance_data.pkl', 'rb') as f:
        cumulative_variance_data = pickle.load(f)
except FileNotFoundError:
    print("Error: cumulative_variance_data.pkl not found. Please run the training cell first.")
    cumulative_variance_data = []

if cumulative_variance_data:
    plt.figure(figsize=(10, 6))

    # Get steps for colormap mapping
    steps = [item['step'] for item in cumulative_variance_data]
    min_step = min(steps)
    max_step = max(steps)

    # Define a colormap
    colormap = cm.turbo # You can choose a different colormap

    # Plot cumulative variance by number of components for each step
    for item in cumulative_variance_data:
        step = item['step']
        cumulative_variance = item['cumulative_variance']
        num_components = len(cumulative_variance)

        # Map step to color
        color = colormap((step - min_step) / (max_step - min_step))

        plt.plot(range(1, num_components + 1), cumulative_variance, linestyle='-', color=color, label=f'Step {step}',alpha=0.5,lw=5)

    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance Ratio')
    plt.title('Cumulative Explained Variance by Number of PCA Components Over Training Steps')
    # Add a colorbar to show the mapping of color to step
    sm = cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=min_step, vmax=max_step))
    sm.set_array([]) # Needed for the colorbar to work
    cbar = plt.colorbar(sm, ax=plt.gca()) # Explicitly provide the current axes
    cbar.set_label('Training Step')

    plt.grid(True)
    plt.xlim([1,20])
    plt.show()
else:
    print("No cumulative variance data to plot.")


#%%
# Create a small batch to demonstrate factored and producted outputs
batch_size_example = 5
seq_len_example = 10
key, key1, key2 = jax.random.split(jax.random.PRNGKey(0), 3) # Use a fixed key for reproducibility

# Generate from tom_quantum
tom_states_example = jnp.repeat(tom_stationary[None, :], batch_size_example, axis=0)
_, tom_inputs_example, _ = generate_data_batch(tom_states_example, tom_quantum, batch_size_example, seq_len_example, key1)

# Generate from mess3
mess3_states_example = jnp.repeat(mess3_stationary[None, :], batch_size_example, axis=0)
_, mess3_inputs_example, _ = generate_data_batch(mess3_states_example, mess3, batch_size_example, seq_len_example, key2)

# Convert to numpy arrays
if isinstance(tom_inputs_example, torch.Tensor):
    tom_arr_example = tom_inputs_example.cpu().numpy()
    mess3_arr_example = mess3_inputs_example.cpu().numpy()
else:
    tom_arr_example = np.array(tom_inputs_example)
    mess3_arr_example = np.array(mess3_inputs_example)


# Combine into product space: token = tom * 3 + mess3
product_tokens_example = tom_arr_example * mess3.vocab_size + mess3_arr_example

print("Example of Factored Outputs (Tom Quantum):")
print(tom_arr_example)

print("\nExample of Factored Outputs (Mess3):")
print(mess3_arr_example)

print("\nExample of Producted Outputs (Combined Tokens):")
print(product_tokens_example)

#%%
# Cell 4: Extract activations from residual stream (Product Space)
from sklearn.decomposition import PCA
import plotly.graph_objects as go

# Generate a batch for analysis
key, key1, key2 = jax.random.split(key, 3)

# Generate product space data
tom_states = jnp.repeat(tom_stationary[None, :], 1000, axis=0)
_, tom_inputs, _ = generate_data_batch(tom_states, tom_quantum, 1000, seq_len, key1)

mess3_states = jnp.repeat(mess3_stationary[None, :], 1000, axis=0)
_, mess3_inputs, _ = generate_data_batch(mess3_states, mess3, 1000, seq_len, key2)

# Convert and combine
if isinstance(tom_inputs, torch.Tensor):
    tom_arr = tom_inputs.cpu().numpy()
    mess3_arr = mess3_inputs.cpu().numpy()
else:
    tom_arr = np.array(tom_inputs)
    mess3_arr = np.array(mess3_inputs)

product_tokens_np = tom_arr * 3 + mess3_arr
tokens = torch.from_numpy(product_tokens_np).long().to(device)

# Run with cache to get all activations
logits, cache = model.run_with_cache(tokens)

# Extract residual stream activations
residual_streams = {
    'embeddings': cache['hook_embed'],
    'layer_0': cache['blocks.0.hook_resid_post'],
    #'layer_1': cache['blocks.1.hook_resid_post'],
}

print("Activation shapes:")
for name, acts in residual_streams.items():
    print(f"  {name}: {acts.shape}")

# Flatten for PCA
activations_flat = {}
for name, acts in residual_streams.items():
    acts_reshaped = acts.reshape(-1, acts.shape[-1]).cpu().numpy()
    activations_flat[name] = acts_reshaped

# Create labels for visualization
token_labels = tokens.flatten().cpu().numpy()
tom_labels = token_labels // 3  # Extract tom component
mess3_labels = token_labels % 3   # Extract mess3 component

print(f"\nTotal points for PCA: {activations_flat['layer_0'].shape[0]}")
print(f"Token distribution: {np.bincount(token_labels)}")

# Cell 5: 3D PCA visualization of residual stream
from sklearn.decomposition import PCA
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px # Import plotly.express for categorical colors

# Perform PCA on the final layer activations (6 components for four 3D plots)
pca = PCA(n_components=6, whiten=True)
pca_coords = pca.fit_transform(activations_flat['layer_0'])

print(f"PCA explained variance ratio: {pca.explained_variance_ratio_}")
print(f"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}")

# Create interactive 3D plots with plotly using subplots (2 rows, 2 columns)
fig = make_subplots(rows=2, cols=2, specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}],
                                          [{'type': 'scatter3d'}, {'type': 'scatter3d'}]],
                    subplot_titles=('Mess3 Coloring: First 3 PCs', 'Mess3 Coloring: Next 3 PCs',
                                    'Tom Quantum Coloring: First 3 PCs', 'Tom Quantum Coloring: Next 3 PCs'))

# Define categorical colors using Plotly Express color sequences
mess3_colors = px.colors.qualitative.Plotly[:mess3.vocab_size] # Get colors for mess3 vocab size
tom_colors = px.colors.qualitative.Set1[:tom_quantum.vocab_size] # Get colors for tom_quantum vocab size

# Map labels to colors
mess3_point_colors = [mess3_colors[label] for label in mess3_labels]
tom_point_colors = [tom_colors[label] for label in tom_labels]


# Top row: Colored by mess3_labels
# Plot 1.1: Mess3, PC1-3
fig.add_trace(go.Scatter3d(
    x=pca_coords[:, 0],
    y=pca_coords[:, 1],
    z=pca_coords[:, 2],
    mode='markers',
    marker=dict(
        size=3,
        color=mess3_point_colors,  # Color by mess3 component using color list
        opacity=0.6
    )
), row=1, col=1)

# Plot 1.2: Mess3, PC4-6
fig.add_trace(go.Scatter3d(
    x=pca_coords[:, 3],
    y=pca_coords[:, 4],
    z=pca_coords[:, 5],
    mode='markers',
    marker=dict(
        size=3,
        color=mess3_point_colors,  # Color by mess3 component using color list
        opacity=0.6,
        showscale=False # Hide colorbar for the second plot in the row
    )
), row=1, col=2)

# Bottom row: Colored by tom_labels
# Plot 2.1: Tom Quantum, PC1-3
fig.add_trace(go.Scatter3d(
    x=pca_coords[:, 0],
    y=pca_coords[:, 1],
    z=pca_coords[:, 2],
    mode='markers',
    marker=dict(
        size=3,
        color=tom_point_colors,  # Color by tom_quantum component using color list
        opacity=0.6
    )
), row=2, col=1)

# Plot 2.2: Tom Quantum, PC4-6
fig.add_trace(go.Scatter3d(
    x=pca_coords[:, 3],
    y=pca_coords[:, 4],
    z=pca_coords[:, 5],
    mode='markers',
    marker=dict(
        size=3,
        color=tom_point_colors,  # Color by tom_quantum component using color list
        opacity=0.6,
        showscale=False # Hide colorbar for the second plot in the row
    )
), row=2, col=2)


fig.update_layout(
    title_text='3D PCA of Residual Stream (Layer 1)',
    scene = dict(
        xaxis_title='PC1',
        yaxis_title='PC2',
        zaxis_title='PC3'),
    scene2 = dict(
        xaxis_title='PC4',
        yaxis_title='PC5',
        zaxis_title='PC6'),
    scene3 = dict( # Scene for the second row, first plot
        xaxis_title='PC1',
        yaxis_title='PC2',
        zaxis_title='PC3'),
    scene4 = dict( # Scene for the second row, second plot
        xaxis_title='PC4',
        yaxis_title='PC5',
        zaxis_title='PC6'),
    height=1200, # Increase height for two rows
    width=1200
)

fig.show()

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Perform PCA on the final layer activations (3 components for 3D)
pca = PCA(n_components=.99)
pca_coords = pca.fit_transform(activations_flat['layer_0'])

# Calculate cumulative explained variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Plot cumulative explained variance
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.title('Cumulative Explained Variance by Number of PCA Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.grid(True)
plt.show()

# Display a sample of the generated tokens
print("Example of generated tokens:")
print(tokens[0, :]) # Display the first sequence in the batch



# Display a sample of the individual components (tom_quantum and mess3)
print("\nExample of individual components:")
print("Tom Quantum component:", tom_inputs[0, :])
print("Mess3 component:", mess3_inputs[0, :])

