# -*- coding: utf-8 -*-
"""Simplexity-Mess3.ipynb

Automatically generated by Colab.

nOriginal file is located at
    https://colab.research.google.com/drive/1lo54AkaiN8cIygtVQC7lUNdbRpRFKQWy
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Installation (skip if already installed)
# %pip -q install --upgrade pip wheel setuptools
# %pip -q install "einops>=0.7.0" "jaxtyping>=0.2.28" "beartype>=0.14" better_abc
# %pip -q install --no-deps "transformer-lens>=2.16.1"
# %pip -q install "git+https://github.com/Astera-org/simplexity.git@MATS_2025_app"


#%%
# Cell 2: Setup
import os
os.environ["JAX_PLATFORMS"] = "cpu"
os.environ["JAX_PLATFORM_NAME"] = "cpu"
import torch
import torch.nn.functional as F
import numpy as np
import jax
#jax.config.update("jax_disable_jit", True)
#jax.config.update("jax_platform_name", "cpu")
import jax.numpy as jnp
from transformer_lens import HookedTransformer, HookedTransformerConfig
from simplexity.generative_processes.torch_generator import generate_data_batch
import argparse, sys, json

from training_and_analysis_utils import train_saes_for_sites, check_cuda_memory
from multipartite_utils import (
    MultipartiteSampler,
    _load_process_stack,
)

import plotly.graph_objects as go
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression

## Speeds up computation on RTX 30 and 40 seris
torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on CuBLAS
torch.backends.cudnn.allow_tf32 = True        # Allow TF32 on CuDNN


#
#%%
# CLI arguments
device_default = "cuda" if torch.cuda.is_available() else "cpu"
parser = argparse.ArgumentParser(description="Train Transformer on mess3 and SAEs")

# HookedTransformerConfig parameters
parser.add_argument("--d_model", type=int, default=128)
parser.add_argument("--n_heads", type=int, default=4)
parser.add_argument("--n_layers", type=int, default=3)
parser.add_argument("--n_ctx", type=int, default=16)
parser.add_argument("--d_vocab", type=int, default=None)
parser.add_argument("--act_fn", type=str, default="relu")
parser.add_argument("--device", type=str, default=device_default)
parser.add_argument("--d_head", type=int, default=32)

# SAE hyperparameters
parser.add_argument("--dict_mul", type=int, default=4)
parser.add_argument("--k", type=int, nargs="+", default=list(range(1, 25)))
#parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=[0.01, 0.015, 0.02, 0.025, 0.05, 0.075] + [round(x, 3) for x in np.arange(0.1, 0.151, 0.005)])
#parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=[1e-3, 0.01, 0.015, 0.02, 0.025, 0.05] + [round(x, 2) for x in np.arange(0.1, 0.651, 0.05)])
parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=None)
parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=None)
parser.add_argument("--input_unit_norm", dest="input_unit_norm", action="store_true", default=True)
parser.add_argument("--no_input_unit_norm", dest="input_unit_norm", action="store_false")
parser.add_argument("--n_batches_to_dead", type=int, default=5)
parser.add_argument("--top_k_aux", type=int, default=None)
parser.add_argument("--aux_penalty", type=float, default=1.0/32.0)
parser.add_argument("--bandwidth", type=float, default=0.001)

# bSAE / AR hyperparameters
parser.add_argument("--ar_lambda_sparse", type=float, nargs="+", default=None,
                    help="Sparse penalty weights for banded covariance SAEs")
parser.add_argument("--ar_lambda_ar", type=float, nargs="+", default=None,
                    help="AR penalty weights for banded covariance SAEs")
parser.add_argument("--ar_cartesian_lambdas", action="store_true",
                    help="If set, take the Cartesian product of lambda_sparse and lambda_ar lists")
parser.add_argument("--ar_p", type=int, default=1, help="Window size p for banded covariance loss")
parser.add_argument("--ar_beta_slope", type=float, default=1.0, help="beta_slope in AR penalty")
parser.add_argument("--ar_delta", type=float, default=1.0, help="delta parameter for smooth L0 sparsity")
parser.add_argument("--ar_epsilon", type=float, default=1e-4, help="epsilon parameter for smooth L0 sparsity")
parser.add_argument("--ar_sparsity_mode", type=str, default="l0", choices=["l0", "l1"],
                    help="Sparsity penalty type for banded covariance SAEs")
parser.add_argument("--ar_use_beta", dest="ar_use_beta", action="store_true", default=True,
                    help="Enable learnable beta coefficients (default)")
parser.add_argument("--ar_no_beta", dest="ar_use_beta", action="store_false",
                    help="Disable learnable beta coefficients")
parser.add_argument("--ar_use_alpha", dest="ar_use_alpha", action="store_true", default=True,
                    help="Enable learnable alpha parameters (default)")
parser.add_argument("--ar_no_alpha", dest="ar_use_alpha", action="store_false",
                    help="Disable learnable alpha parameters")

# SAE training loop controls
parser.add_argument("--sae_steps", type=int, default=5000)
parser.add_argument("--sae_batch_size", type=int, default=1024)
parser.add_argument("--sae_seq_len", type=int, default=None)
parser.add_argument("--seq_len", type=int, default=None, help="Sequence length used for analysis/visualization batches; defaults to n_ctx")
parser.add_argument("--sae_output_dir", type=str, default="outputs/saes/multipartite_001", help="Directory to save trained SAEs and metrics")
parser.add_argument("--sae_learning_rate", type=float, default=3e-4, help="Base learning rate for SAE optimizers")
parser.add_argument("--sae_weight_decay", type=float, default=0.0, help="Weight decay for SAE optimizers")
parser.add_argument("--sae_beta1", type=float, default=0.9, help="Adam beta1 parameter for SAE optimizers")
parser.add_argument("--sae_beta2", type=float, default=0.99, help="Adam beta2 parameter for SAE optimizers")
parser.add_argument("--sae_scheduler", type=str, default="cosine", choices=["none", "cosine", "linear"], help="Learning rate scheduler strategy for SAEs")
parser.add_argument("--sae_scheduler_warmup_steps", type=int, default=500, help="Warmup steps before applying the SAE scheduler")
parser.add_argument("--sae_scheduler_final_ratio", type=float, default=0.2, help="Final LR ratio for SAE schedulers")
parser.add_argument("--sae_grad_clip_norm", type=float, default=None, help="Gradient clipping norm for SAE training (None keeps default clamp)")
parser.add_argument("--sae_early_stopping_patience", type=int, default=0, help="Patience for SAE early stopping based on EMA loss (0 disables)")
parser.add_argument("--sae_early_stopping_delta", type=float, default=5e-5, help="Required EMA improvement to reset SAE early stopping patience")
parser.add_argument("--sae_early_stopping_beta", type=float, default=0.98, help="EMA smoothing factor for SAE early stopping")
parser.add_argument("--sae_early_stopping_min_steps", type=int, default=1000, help="Minimum SAE steps before early stopping may trigger")
parser.add_argument("--sae_log_interval", type=int, default=200, help="Iterations between SAE progress updates")

# Model loading
parser.add_argument("--load_model", type=str, default="outputs/checkpoints/multipartite_001/checkpoint_step_500000_final.pt", help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--process_config", type=str, default="process_configs.json", help="Path to JSON describing a stack of generative processes or mapping of named configurations")
parser.add_argument("--process_config_name", type=str, default=None, help="Key within --process_config when the file contains multiple named configurations")

# Parse known to be notebook-friendly
args, _ = parser.parse_known_args()

if (args.ar_lambda_sparse is None) != (args.ar_lambda_ar is None):
    raise ValueError("Both --ar_lambda_sparse and --ar_lambda_ar must be provided together, or both omitted.")

_process_cfg_raw, components, data_source = _load_process_stack(args, {})

#%%
type_counts: dict[str, int] = {}
for comp in components:
    type_counts[comp.process_type] = type_counts.get(comp.process_type, 0) + 1
counts_str = ", ".join(f"{ptype}:{count}" for ptype, count in type_counts.items())
config_label = args.process_config_name if args.process_config_name else "<unnamed>"
print(f"Loaded process config '{config_label}' from {args.process_config} → {counts_str}")

if isinstance(data_source, MultipartiteSampler):
    vocab_size = data_source.vocab_size
    num_states = data_source.belief_dim
    print(
        f"Multipartite process with components {[c.name for c in data_source.components]}"
        f" → vocab_size={vocab_size}, belief_dim={num_states}"
    )
else:
    vocab_size = data_source.vocab_size
    num_states = data_source.num_states
    print(f"Process: vocab_size={vocab_size}, states={num_states}")
# Create TransformerLens model
device = args.device
cfg = HookedTransformerConfig(
    d_model=args.d_model,
    n_heads=args.n_heads,
    n_layers=args.n_layers,
    n_ctx=args.n_ctx,
    d_vocab=(args.d_vocab if args.d_vocab is not None else vocab_size),
    act_fn=args.act_fn,
    device=device,
    d_head=args.d_head,
)
model = HookedTransformer(cfg)
print(f"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}")

# Define a default sequence length for downstream analysis/visualization
seq_len = args.seq_len if args.seq_len is not None else cfg.n_ctx


#%%
## Train or load model

key = jax.random.PRNGKey(42)
# Cell 3: Train (or load)
losses = []
if args.load_model is None:
    raise ValueError("Don't want to train model from scratch here")
    if isinstance(data_source, MultipartiteSampler):
        raise ValueError(
            "Training from scratch is not implemented for multipartite stacks; "
            "please provide --load_model pointing to a pretrained checkpoint."
        )
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    batch_size, seq_len = 1024, cfg.n_ctx
    # Get stationary distribution for initial states
    stationary = data_source.initial_state
    # Create the tqdm progress bar object
    progress_bar = tqdm(range(25000), desc="Training", miniters=1_000, disable=not sys.stderr.isatty())
    for step in progress_bar:
        key, subkey = jax.random.split(key)
        gen_states = jnp.repeat(stationary[None, :], batch_size, axis=0)
        _, inputs, _ = generate_data_batch(gen_states, data_source, batch_size, seq_len+1, subkey)
        if isinstance(inputs, torch.Tensor):
            tokens = inputs.long().to(device)
        else:
            tokens = torch.from_numpy(np.array(inputs)).long().to(device)
        loss = model(tokens, return_type="loss")
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        progress_bar.set_description(f"Training (Loss: {loss.item():.4f})", refresh=False)
    print(f"\nFinal loss: {losses[-1]:.4f} (started at {losses[0]:.4f})")

    # Quick visualization
    plt.plot(losses)
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.show()

    ## Save trained model
    os.makedirs("outputs", exist_ok=True)
    save_path = os.path.join("outputs", "mess3_transformer.pt")
    checkpoint = {
        "state_dict": model.state_dict(),
        "config": cfg.to_dict() if hasattr(cfg, "to_dict") else vars(cfg),
    }
    torch.save(checkpoint, save_path)
    print(f"Saved trained model to {save_path}")
else:
    ckpt = torch.load(args.load_model, map_location=device, weights_only=False)
    cfg_loaded = None
    if isinstance(ckpt.get("config"), dict):
        cfg_loaded = HookedTransformerConfig.from_dict(ckpt["config"])
        model = HookedTransformer(cfg_loaded).to(device)
    state_dict = ckpt.get("state_dict") or ckpt.get("model_state_dict")
    if state_dict is None:
        raise KeyError("Checkpoint missing Transformer state ")
    if cfg_loaded is None:
        model = HookedTransformer(cfg).to(device)
    model.load_state_dict(state_dict)  # type: ignore[arg-type]
    model.eval()
    print(f"Loaded model from {args.load_model}")


#%% 
## Train SAEs

# Train SAEs at the key residual stream points used below
site_to_hook = {"embeddings": "hook_embed"}
for layer_idx in range(cfg.n_layers):
    site_to_hook[f"layer_{layer_idx}"] = f"blocks.{layer_idx}.hook_resid_post"

sequence_saes = {}
true_coords_saes = {}
metrics_summary = {}
print("Training SAEs for all sites in one pass")
sequence_saes, true_coords_saes, metrics_summary = train_saes_for_sites(
    site_to_hook,
    model=model,
    data_source=data_source,
    cfg=cfg,
    device=device,
    rng_key=key,
    steps=args.sae_steps,
    batch_size=args.sae_batch_size,
    seq_len=(args.sae_seq_len if args.sae_seq_len is not None else cfg.n_ctx - 1),
    k_values=args.k,
    lambda_values_seq=(args.l1_coeff_seq if args.l1_coeff_seq is not None else None),
    lambda_values_beliefs=(args.l1_coeff_beliefs if args.l1_coeff_beliefs is not None else None),
    dict_mul=args.dict_mul,
    input_unit_norm=args.input_unit_norm,
    n_batches_to_dead=args.n_batches_to_dead,
    top_k_aux=args.top_k_aux,
    aux_penalty=args.aux_penalty,
    bandwidth=args.bandwidth,
    belief_dim=num_states,
    sae_output_dir=args.sae_output_dir,
    sae_learning_rate=args.sae_learning_rate,
    sae_weight_decay=args.sae_weight_decay,
    sae_beta1=args.sae_beta1,
    sae_beta2=args.sae_beta2,
    sae_scheduler=args.sae_scheduler,
    sae_scheduler_warmup_steps=args.sae_scheduler_warmup_steps,
    sae_scheduler_final_ratio=args.sae_scheduler_final_ratio,
    sae_grad_clip_norm=args.sae_grad_clip_norm,
    sae_early_stopping_patience=args.sae_early_stopping_patience,
    sae_early_stopping_delta=args.sae_early_stopping_delta,
    sae_early_stopping_beta=args.sae_early_stopping_beta,
    sae_early_stopping_min_steps=args.sae_early_stopping_min_steps,
    sae_log_interval=args.sae_log_interval,
    ar_lambda_sparse=args.ar_lambda_sparse,
    ar_lambda_ar=args.ar_lambda_ar,
    ar_cartesian_lambdas=args.ar_cartesian_lambdas,
    ar_p=args.ar_p,
    ar_beta_slope=args.ar_beta_slope,
    ar_delta=args.ar_delta,
    ar_epsilon=args.ar_epsilon,
    ar_sparsity_mode=args.ar_sparsity_mode,
    ar_use_beta=args.ar_use_beta,
    ar_use_alpha=args.ar_use_alpha,
)

#%%

# Save the entire metrics summary once across all layers
os.makedirs(args.sae_output_dir, exist_ok=True)
metrics_path = os.path.join(args.sae_output_dir, "metrics_summary.json")
with open(metrics_path, "w") as f:
    json.dump(metrics_summary, f, indent=2)
print(f"Saved SAE metrics summary to {metrics_path}")

#%%
