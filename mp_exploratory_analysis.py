# -*- coding: utf-8 -*-
"""Simplexity-Mess3.ipynb

Automatically generated by Colab.

nOriginal file is located at
    https://colab.research.google.com/drive/1lo54AkaiN8cIygtVQC7lUNdbRpRFKQWy
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Installation (skip if already installed)
# %pip -q install --upgrade pip wheel setuptools
# %pip -q install "einops>=0.7.0" "jaxtyping>=0.2.28" "beartype>=0.14" better_abc
# %pip -q install --no-deps "transformer-lens>=2.16.1"
# %pip -q install "git+https://github.com/Astera-org/simplexity.git@MATS_2025_app"


#%%
# Cell 2: Setup
import argparse
from collections import defaultdict
from itertools import combinations
from math import comb
from typing import Any, Mapping, Sequence
import json
import sys
import gc

import jax
import jax.numpy as jnp
import numpy as np
import torch
from sklearn.decomposition import PCA
from tqdm.auto import tqdm
from transformer_lens import HookedTransformer, HookedTransformerConfig
from BatchTopK.sae import TopKSAE, VanillaSAE

from simplexity.generative_processes.torch_generator import generate_data_batch

from multipartite_utils import (
    MultipartiteSampler,
    _load_process_stack,
)
from training_and_analysis_utils import (
    plot_pca_subplots,
    evaluate_belief_state_linear_models,
    project_vectors_onto_simplex,
    project_simplex3_to_2d,
    plot_mess3_belief_grid,
    plot_tom_quantum_coherence,
    plot_tom_quantum_coherence_grid,
    _aggregate_layer_metrics,
    _print_residual_summary_table,
    _print_combined_regression_report,
    load_metrics_summary,
    _load_sae_checkpoints,
)
from epdf_utils import (
    build_mp_latent_epdfs,
    plot_mp_epdfs,
    save_latent_epdfs,
    load_latent_epdfs,
    build_epdfs_from_sae_and_beliefs,
    plot_epdfs_to_directory,
)
import os
import matplotlib.pyplot as plt
from matplotlib import colors as mcolors
import glob
import re

jax.config.update("jax_platform_name", "cpu")


def log_memory_usage(label: str, verbose: bool = False):
    """Log current memory usage if verbose flag is enabled."""
    if not verbose:
        return
    try:
        import psutil
        process = psutil.Process()
        mem_info = process.memory_info()
        mem_mb = mem_info.rss / 1024 / 1024
        print(f"[Memory] {label}: {mem_mb:.1f} MB")
    except ImportError:
        # psutil not available, skip logging
        pass


PRESET_PROCESS_CONFIGS = {
    "single_mess3": [
        {"type": "mess3", "params": {"x": 0.1, "a": 0.7}},
    ],
    "3xmess3_2xtquant": [
        {
            "type": "tom_quantum",
            "instances": [
                #{"alpha": 0.8, "beta": float(1.3)},
                {"alpha": 1.0, "beta": float(np.sqrt(51))},
                {"alpha": 1.0, "beta": float(np.sqrt(51))},
            ],
        },
        {
            "type": "mess3",
            "instances": [
                {"x": 0.10, "a": 0.50},
                {"x": 0.25, "a": 0.80},
                {"x": 0.40, "a": 0.20},
            ],
        },
    ],
}


#%%
# CLI arguments
device_default = "cuda" if torch.cuda.is_available() else "cpu"
parser = argparse.ArgumentParser(description="Train Transformer on mess3 and SAEs", allow_abbrev=False)

# HookedTransformerConfig parameters
parser.add_argument("--d_model", type=int, default=128)
parser.add_argument("--n_heads", type=int, default=4)
parser.add_argument("--n_layers", type=int, default=3)
parser.add_argument("--n_ctx", type=int, default=16)
parser.add_argument("--d_vocab", type=int, default=None)
parser.add_argument("--act_fn", type=str, default="relu")
parser.add_argument("--device", type=str, default=device_default)
parser.add_argument("--seed", type=int, default=0, help="Random seed for analysis sampling")
parser.add_argument("--d_head", type=int, default=32)

# SAE hyperparameters
# parser.add_argument("--dict_mul", type=int, default=4)
# parser.add_argument("--k", type=int, nargs="+", default=[3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 19, 22, 25])
# #parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=[0.01, 0.015, 0.02, 0.025, 0.05, 0.075] + [round(x, 3) for x in np.arange(0.1, 0.151, 0.005)])
# #parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=[1e-3, 0.01, 0.015, 0.02, 0.025, 0.05] + [round(x, 2) for x in np.arange(0.1, 0.651, 0.05)])
# parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=[0.001, 0.005, 0.01, 0.05, 0.1, 0.15])
# parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=None)
# parser.add_argument("--input_unit_norm", dest="input_unit_norm", action="store_true", default=True)
# parser.add_argument("--no_input_unit_norm", dest="input_unit_norm", action="store_false")
# parser.add_argument("--n_batches_to_dead", type=int, default=5)
# parser.add_argument("--top_k_aux", type=int, default=None)
# parser.add_argument("--aux_penalty", type=float, default=0.03125)
parser.add_argument("--bandwidth", type=float, default=0.001)

# SAE training loop controls
parser.add_argument("--seq_len", type=int, default=None, help="Sequence length used for analysis/visualization batches; defaults to n_ctx",)
parser.add_argument("--analysis_batch_size", type=int, default=8192, help="Batch size for exploratory sampling from the multipartite stack",)
parser.add_argument("--fig_out_dir", type=str, default="outputs/reports/multipartite_003e", help="Directory to save matplotlib figures")
parser.add_argument("--plot_pcas", action="store_true", default=False, help="Plot all the 2 and 3 combinations of PCs")

parser.add_argument("--linear_prediction_layer", type=str, default="layer_2", help="Layer name to use for linear-prediction visualizations, e.g. 'layer_2'")
parser.add_argument("--sae_folder", type=str, default="outputs/saes/multipartite_003e", help="Folder containing SAE checkpoints for EPDF analysis")
parser.add_argument("--metrics_summary", type=str, default=None, help="Optional path to metrics_summary.json; defaults to <sae_folder>/metrics_summary.json")
parser.add_argument("--build_epdfs", action="store_true", default=False, help="Generate multipartite EPDF visualizations")
parser.add_argument("--epdf_output_dir", type=str, default=None, help="Output directory for EPDF figures; defaults to <fig_out_dir>/epdfs")
parser.add_argument("--epdf_latent_min_fraction", type=float, default=0.05, help="Minimum latent activity fraction required to plot an EPDF")
parser.add_argument("--epdf_activation_threshold", type=float, default=1e-6, help="Activation magnitude threshold for including samples in EPDFs")
parser.add_argument("--epdf_max_points", type=int, default=2000, help="Maximum samples per component when plotting EPDFs")
parser.add_argument("--epdf_cache_dir", type=str, default="outputs/reports/multipartite_002/epdfs/cache", help="Directory to save/load serialized latent EPDFs")
parser.add_argument("--epdf_use_cache", action="store_true", default=False, help="Load cached EPDFs when available instead of recomputing")
parser.add_argument("--epdf_sites", type=str, nargs="+", default=None, help="Subset of sites to process for EPDFs (e.g., layer_0 layer_1); if None, processes all sites")
parser.add_argument("--epdf_plot_mode", type=str, default="both", choices=["both", "all_only", "per_latent_only"], help="Which EPDF plots to generate: 'both' (all+per-latent), 'all_only', or 'per_latent_only'")
parser.add_argument("--max_latents_per_sae", type=int, default=None, help="Maximum number of latents to process per SAE (uses most active); if None, processes all filtered latents")
parser.add_argument("--verbose_memory", action="store_true", default=False, help="Log memory usage at key checkpoints during EPDF generation")

# Model loading
#parser.add_argument("--load_model", type=str, default=None, help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--load_model", type=str, default="outputs/checkpoints/multipartite_003/checkpoint_step_6000_best.pt", help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--process_config", type=str, default="process_configs.json", help="Path to JSON describing a stack of generative processes or a mapping of named configurations")
parser.add_argument("--process_config_name", type=str, default="3xmess3_2xtquant_003", help="Key within --process_config when the file stores multiple named configurations")
parser.add_argument("--process_preset", type=str, default=None, help="Named preset for generative process configuration")
parser.add_argument("--pc_inds", type=str, default="pc_inds.json", help="JSON file describing the PCA indices for each component")

# Parse known to be notebook-friendly
args, _ = parser.parse_known_args()

args.plot_pcas = True
args.build_epdfs = False

if args.process_config and args.process_preset:
    parser.error("Specify at most one of --process_config or --process_preset")
if args.process_config_name and not args.process_config:
    parser.error("--process_config_name requires --process_config")

_process_config_raw, components, data_source = _load_process_stack(args, PRESET_PROCESS_CONFIGS)
sampler: MultipartiteSampler | None
if len(components) == 1:
    data_source = components[0].process
    sampler = None
else:
    sampler = MultipartiteSampler(components)
    data_source = sampler


# Handle --pc_inds argument: load JSON mapping of layer/component to PC indices
layer_pc_inds_map = {}
if args.pc_inds is not None:
    with open(args.pc_inds, "r") as f:
        layer_pc_inds_map = json.load(f)

linear_prediction_layer = args.linear_prediction_layer

if isinstance(data_source, MultipartiteSampler):
    vocab_size = data_source.vocab_size
    num_states = data_source.belief_dim
    print(
        f"Multipartite process with components {[c.name for c in data_source.components]}"
        f" → vocab_size={vocab_size}, belief_dim={num_states}"
    )
else:
    vocab_size = data_source.vocab_size
    num_states = data_source.num_states
    print(f"Process: vocab_size={vocab_size}, states={num_states}")

# Create TransformerLens model
device = args.device
cfg = HookedTransformerConfig(
    d_model=args.d_model,
    n_heads=args.n_heads,
    n_layers=args.n_layers,
    n_ctx=args.n_ctx,
    d_vocab=(args.d_vocab if args.d_vocab is not None else vocab_size),
    act_fn=args.act_fn,
    device=device,
    d_head=args.d_head,
)
model = HookedTransformer(cfg)
print(f"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}")

# Define a default sequence length for downstream analysis/visualization
seq_len = args.seq_len if args.seq_len is not None else cfg.n_ctx


#%%
# Load metrics summary (for active latent metadata)

metrics_summary_path = args.metrics_summary
if metrics_summary_path is None and args.sae_folder is not None:
    metrics_summary_path = os.path.join(args.sae_folder, "metrics_summary.json")

metrics_summary = load_metrics_summary(metrics_summary_path) if metrics_summary_path else None

active_latents: dict[tuple[str, ...], dict[str, list[float]]] = {}
if metrics_summary:
    for site_name, site_data in metrics_summary.items():
        if not isinstance(site_data, dict):
            continue
        sequence_group = site_data.get("sequence", {})
        for sae_type in ("top_k", "vanilla"):
            group = sequence_group.get(sae_type, {})
            for param_name, record in group.items():
                inds = record.get("active_latents_last_quarter", {})
                if inds:
                    try:
                        normalized = {str(k): v for k, v in inds.items()}
                    except Exception:
                        normalized = inds
                    key = (site_name, "sequence", sae_type, param_name)
                    active_latents[key] = normalized


#%%
## Load model

key = jax.random.PRNGKey(42)
# Cell 3: Train (or load)
losses = []
if args.load_model is None:
    raise ValueError("Must provide --load_model")

ckpt = torch.load(args.load_model, map_location=device, weights_only=False)
cfg_loaded = None
if isinstance(ckpt.get("config"), dict):
    cfg_loaded = HookedTransformerConfig.from_dict(ckpt["config"])
    model = HookedTransformer(cfg_loaded).to(device)
state_dict = ckpt.get("state_dict") or ckpt.get("model_state_dict")
if state_dict is None:
    raise KeyError("Checkpoint missing Transformer state dict")
if cfg_loaded is None:
    model = HookedTransformer(cfg).to(device)
model.load_state_dict(state_dict)  # type: ignore[arg-type]
#optimizer.load_state_dict(ckpt['optimizer_state_dict'])
losses = ckpt['losses']
model.eval()
print(f"Loaded model from {args.load_model}")


#%%
# ==== Plot Moving Average of Losses ==== #
#########################################
# Visualization of loss
# Calculate moving average
window_size = 50 # You can adjust the window size
moving_average = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')

# Plot original loss with transparency
plt.figure()
plt.plot(losses, alpha=0.5, label='Original Loss') 
# Plot moving average
plt.plot(range(window_size - 1, len(losses)), moving_average,
         label=f'Moving Average (window={window_size})') 

plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training Loss on Product Space (multiple processes)')

# Calculate ylim based on percentiles
ylim_min = 0.9999 * np.min(losses)
ylim_max = ylim_min + 0.25 * (np.max(losses) - ylim_min)
plt.ylim([ylim_min, ylim_max])
#plt.yscale('log')  # Set y-axis to logarithmic scale
# Add vertical dotted lines
plt.axvline(x=1000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=5000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=10000, color='gray', linestyle=':', label='10 dims')
plt.axvline(x=24000, color='gray', linestyle=':', label='10 dims')
plt.legend()
os.makedirs(args.fig_out_dir, exist_ok=True)
plt.tight_layout()
plt.savefig(os.path.join(args.fig_out_dir, 'loss_moving_average.png'))

plt.axvline(x=2000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=3000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=4000, color='gray', linestyle=':', label='6 dims')
plt.xlim(0,80000)
plt.savefig(os.path.join(args.fig_out_dir, 'loss_moving_average_xlim_100k.png'))
plt.close()


#%%
# ==== Generate PCA of Residual Stream ==== #
############################################
# Generate a batch for analysis using the multipartite sampler utilities

analysis_batch_size = args.analysis_batch_size
plot_pcas = args.plot_pcas

# Sample one analysis batch
component_token_arrays: list[np.ndarray] = []
component_belief_arrays: list[np.ndarray] = []


if isinstance(data_source, MultipartiteSampler):
    key, belief_states, product_tokens, component_observations = data_source.sample(
        key, analysis_batch_size, seq_len
    )
    product_vocab_size = data_source.vocab_size
    tokens = torch.from_numpy(np.array(product_tokens)).long().to(device)
    beliefs_np = np.array(belief_states)
    component_token_arrays = [np.array(obs) for obs in component_observations]
    cursor = 0
    for dim in data_source.component_state_dims:
        component_belief_arrays.append(beliefs_np[..., cursor: cursor + dim])
        cursor += dim
    component_vocab_sizes = list(data_source.component_vocab_sizes)
    ordered_components = list(data_source.components)
else:
    raise TypeError("Data source is not a MultipartiteSampler")
    # key, subkey = jax.random.split(key)
    # init_states = jnp.repeat(data_source.initial_state[None, :], analysis_batch_size, axis=0)
    # beliefs_np, observations, _ = generate_data_batch(
    #     init_states, data_source, analysis_batch_size, seq_len, subkey
    # )
    # product_vocab_size = data_source.vocab_size
    # tokens = torch.from_numpy(np.array(observations)).long().to(device)
    # component_token_arrays = [np.array(observations)]
    # component_belief_arrays = [np.array(beliefs_np)]
    # component_vocab_sizes = [data_source.vocab_size]

# Component metadata (name/type/vocab/belief_dim)
component_metadata: list[dict[str, object]] = []
if isinstance(data_source, MultipartiteSampler):
    component_iter = enumerate(ordered_components)
else:
    component_iter = enumerate(components)

for idx, comp in component_iter:
    comp_type = getattr(comp, "process_type", comp.name.split("_")[0])
    vocab_size = component_vocab_sizes[idx] if idx < len(component_vocab_sizes) else getattr(comp.process, "vocab_size", product_vocab_size)
    belief_dim = int(component_belief_arrays[idx].shape[-1]) if idx < len(component_belief_arrays) else int(getattr(comp.process, "num_states", 0))
    component_metadata.append(
        {"name": comp.name, "type": comp_type, "vocab_size": int(vocab_size), "belief_dim": belief_dim}
    )

#%%
print(f"{len(component_belief_arrays)=}")
print(f"{component_belief_arrays[0].shape=}")


#%%

print("Analysis batch summary:")
print(f"  product tokens -> batch={tokens.shape[0]}, seq={tokens.shape[1]}, vocab={product_vocab_size}")
for meta, obs in zip(component_metadata, component_token_arrays):
    print(f"  {meta['name']} ({meta['type']}): tokens shape {obs.shape}, vocab={meta['vocab_size']}, belief_dim={meta['belief_dim']}")

# Run model & collect residual stream activations
logits, cache = model.run_with_cache(tokens)
residual_streams = {"embeddings": cache["hook_embed"]}
for i in range(args.n_layers):
    residual_streams[f"layer_{i}"] = cache[f"blocks.{i}.hook_resid_post"]
try:
    residual_streams["ln_final"] = cache["ln_final.hook_normalized"]
except KeyError:
    pass

print("Activation shapes:")
for name, acts in residual_streams.items():
    print(f"  {name}: {acts.shape}")


# ---- PCA data prep (select specific token positions then flatten) ----
token_inds = [4, 9, 14]  # positions (0-indexed) to visualize / probe
activations_flat: dict[str, np.ndarray] = {}
for name, acts in residual_streams.items():
    # acts: [batch, seq, d_model] -> take columns at token_inds -> [batch, len(token_inds), d_model]
    # then flatten to [batch*len(token_inds), d_model] in C-order (row-major)
    acts_sel = torch.Tensor(acts)[:, token_inds, :].reshape(-1, acts.shape[-1]).cpu().numpy()
    activations_flat[name] = acts_sel

print("Flattened activation shapes:")
for name, acts in activations_flat.items():
    print(f"  {name}: {acts.shape}")

# Choose which site to PCA
# Getting this as an argument from the command line
# linear_prediction_layer = f"layer_{args.n_layers - 1}" if args.n_layers > 0 else "embeddings"
if linear_prediction_layer not in activations_flat:
    available_layers = ", ".join(sorted(activations_flat.keys()))
    raise KeyError(f"Requested layer '{linear_prediction_layer}' not found. Available: {available_layers}")

layer_pca_models: dict[str, PCA] = {}
pca_coords: np.ndarray | None = None

for layer_name, acts_flat in activations_flat.items():
    n_components = min(15, acts_flat.shape[0], acts_flat.shape[1])
    if n_components < 2:
        print(f"Skipping PCA for {layer_name}: insufficient samples/features (n_components={n_components})")
        continue

    layer_pca = PCA(n_components=n_components, whiten=True)
    layer_coords = layer_pca.fit_transform(acts_flat)
    layer_pca_models[layer_name] = layer_pca
    
    if layer_name == linear_prediction_layer:
        pca_coords = layer_coords

if linear_prediction_layer not in layer_pca_models:
    raise ValueError(f"Failed to fit PCA for target layer '{linear_prediction_layer}'")

pca = layer_pca_models[linear_prediction_layer]
if pca_coords is None:
    target_acts = activations_flat[linear_prediction_layer]
    pca_coords = pca.transform(target_acts)

print(f"PCA ({linear_prediction_layer}) explained variance ratio: {pca.explained_variance_ratio_}")
print(f"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}")


# --- build labels with the sampler's true order preserved ---
tok_idx_t = torch.tensor(token_inds, device=device)

def flatten_bt_labels(obs_np: np.ndarray) -> np.ndarray:
    # obs_np: (B, T) ints from the sampler
    t = torch.from_numpy(np.asarray(obs_np)).to(device).long()   # (B, T)
    sel = t.index_select(1, tok_idx_t)                           # (B, K)
    return sel.contiguous().view(-1).cpu().numpy().astype(int)   # (B*K,)

# Pair observations with their metadata to preserve alignment
mess3_label_entries: list[tuple[str, np.ndarray]] = []
tom_label_entries: list[tuple[str, np.ndarray]] = []
for meta, obs_np in zip(component_metadata, component_token_arrays):
    flat = flatten_bt_labels(obs_np)
    if meta["type"] == "mess3":
        mess3_label_entries.append((meta["name"], flat))
    elif meta["type"] == "tom_quantum":
        tom_label_entries.append((meta["name"], flat))

# palettes
_mess3_base = ["#ff595e", "#1982c4", "#8ac926"]
_tom_base   = ["#ff1744", "#00c853", "#2962ff", "#ffd600"]
MESS3_RGB = np.array([mcolors.to_rgb(hex_code) for hex_code in _mess3_base], dtype=np.float64)
TOM_RGB = np.array([mcolors.to_rgb(hex_code) for hex_code in ["#ff1744", "#00c853", "#2962ff", "#ffd600"]], dtype=np.float64)

mess3_vocab = max((int(m["vocab_size"]) for m in component_metadata if m["type"]=="mess3"), default=0)
tom_vocab   = max((int(m["vocab_size"]) for m in component_metadata if m["type"]=="tom_quantum"), default=0)
mess3_label_to_color   = {i: _mess3_base[i % len(_mess3_base)] for i in range(mess3_vocab)}
tom_label_to_color     = {i: _tom_base[i % len(_tom_base)]     for i in range(tom_vocab)}

# Derive deterministic ordering consistent with the original configuration
mess3_order = [comp.name for comp in components if getattr(comp, "process_type", comp.name.split("_")[0]) == "mess3"]
tom_order   = [comp.name for comp in components if getattr(comp, "process_type", comp.name.split("_")[0]) == "tom_quantum"]

mess3_label_map = {name: lab for name, lab in mess3_label_entries}
tom_label_map   = {name: lab for name, lab in tom_label_entries}

mess3_point_colors = [
    [mess3_label_to_color[int(v)] for v in mess3_label_map[name]]
    for name in mess3_order
]
tom_point_colors = [
    [tom_label_to_color[int(v)] for v in tom_label_map[name]]
    for name in tom_order
]

# keep flattened label arrays for downstream checks if needed
mess3_labels_flat = [mess3_label_map[name] for name in mess3_order]
tom_labels_flat   = [tom_label_map[name]   for name in tom_order]

# Cache per-component belief/prediction geometry for later composite figures
belief_plot_cache: dict[str, dict[int, dict[str, np.ndarray]]] = defaultdict(dict)

# sanity
N = pca_coords.shape[0]
for i, arr in enumerate(mess3_point_colors):
    assert len(arr) == N, f"mess3[{i}] color length {len(arr)} != {N}"
for i, arr in enumerate(tom_point_colors):
    assert len(arr) == N, f"tom[{i}] color length {len(arr)} != {N}"



# ==== Belief-State Linear Probing ============================================ #

# Use the same positions as PCA sampling to keep comparisons consistent
evaluation_positions = token_inds
final_layer_name = f"layer_{args.n_layers - 1}"
if final_layer_name not in residual_streams:
    available = ", ".join(sorted(residual_streams.keys()))
    raise KeyError(f"Expected residual stream '{final_layer_name}' not found. Have: {available}")

final_layer_activations = residual_streams[final_layer_name]
with torch.no_grad():
    ln_final_activations = model.ln_final(final_layer_activations)
residual_streams["ln_final"] = ln_final_activations

linear_prediction_layer = args.linear_prediction_layer


regression_common_kwargs = dict(
    seq_positions=evaluation_positions,
    skip_dims_by_type={"tom_quantum": [0]},
    postprocess_by_type={
        "mess3": lambda arr: project_vectors_onto_simplex(arr, axis=-1),
        # Intentionally skip Tom Quantum clamping so we can inspect raw predictions.
    },
    store_predictions=True,
)

probe_layers: list[tuple[str, torch.Tensor]] = []
if "embeddings" in residual_streams:
    probe_layers.append(("embeddings", residual_streams["embeddings"]))
for i in range(args.n_layers):
    label = f"layer_{i}"
    if label in residual_streams:
        probe_layers.append((label, residual_streams[label]))
probe_layers.append(("ln_final", ln_final_activations))


layer_results = {}
for layer_label, acts in probe_layers:
    layer_pca = layer_pca_models.get(layer_label)
    layer_pc_inds = layer_pc_inds_map.get(layer_label, None)
    metrics = evaluate_belief_state_linear_models(
        activations=acts,
        component_belief_arrays=component_belief_arrays,
        component_metadata=component_metadata,
        pca=layer_pca,
        pc_inds=layer_pc_inds,
        **regression_common_kwargs,
    )
    layer_results[layer_label] = metrics

_print_combined_regression_report(layer_results, evaluation_positions, component_metadata)

_print_residual_summary_table(
    layer_results,
    final_layer_name,
    "ln_final",
    evaluation_positions,
    component_metadata,
)

# Choose the layer for the plots
belief_regression_metrics = layer_results.get(linear_prediction_layer, layer_results.get(final_layer_name, {}))


#%%
# ==== Multipartite EPDF Visualizations ==================================== #

# args.build_epdfs = True
# args.epdf_output_dir = None
# args.epdf_cache_dir = "outputs/reports/multipartite_002/epdfs/cache"
# args.epdf_use_cache = True
# args.epdf_max_points = 2000
# args.sae_folder = "outputs/saes/multipartite_002"

if args.build_epdfs:
    if args.sae_folder is None:
        print("Skipping EPDF generation: --sae_folder not provided")
    else:
        print("Generating Multipartite ePDFs")
        epdf_output_dir = args.epdf_output_dir or os.path.join(args.fig_out_dir, "epdfs")
        os.makedirs(epdf_output_dir, exist_ok=True)

        loaded_saes = _load_sae_checkpoints(args.sae_folder, device)

        # Filter to requested sites if specified
        if args.epdf_sites is not None:
            original_sites = set(loaded_saes.keys())
            loaded_saes = {
                site: saes for site, saes in loaded_saes.items()
                if site in args.epdf_sites
            }
            filtered_sites = set(loaded_saes.keys())
            if not loaded_saes:
                print(f"Warning: None of the requested sites {args.epdf_sites} found in SAE checkpoints")
                print(f"Available sites: {sorted(original_sites)}")
            else:
                excluded = original_sites - filtered_sites
                if excluded:
                    print(f"Filtering to sites: {sorted(filtered_sites)}")
                    print(f"Excluded sites: {sorted(excluded)}")

        if not loaded_saes:
            print(f"No SAE checkpoints found in {args.sae_folder}; skipping EPDFs")
        else:
            cache_dir = args.epdf_cache_dir
            if cache_dir is None and args.epdf_use_cache:
                cache_dir = os.path.join(epdf_output_dir, "cache")
            if cache_dir is not None:
                os.makedirs(cache_dir, exist_ok=True)

            component_meta_map = {str(meta["name"]): meta for meta in component_metadata}
            component_order = [str(meta["name"]) for meta in component_metadata]

            flattened_component_beliefs: dict[str, np.ndarray] = {}
            for meta, beliefs in zip(component_metadata, component_belief_arrays, strict=True):
                comp_name = str(meta["name"])
                if token_inds:
                    selected = beliefs[:, token_inds, :]
                else:
                    selected = beliefs
                flattened_component_beliefs[comp_name] = selected.reshape(-1, selected.shape[-1])

            log_memory_usage("Before EPDF generation", args.verbose_memory)

            sae_result_counter = 0
            site_items = list(loaded_saes.items())
            for site_name, sae_map in tqdm(
                site_items,
                desc="EPDF sites",
                leave=True,
                #disable=not sys.stderr.isatty(),
            ):
                acts_np = activations_flat.get(site_name)
                if acts_np is None:
                    continue

                acts_tensor = torch.from_numpy(acts_np).to(device)

                sae_items = list(sae_map.items())
                for (sae_type, param_token), ckpt in tqdm(
                    sae_items,
                    desc=f"{site_name} SAEs",
                    leave=True,
                    #disable=not sys.stderr.isatty(),
                ):
                    if sae_type == "top_k":
                        sae_class = TopKSAE
                        active_key = (site_name, "sequence", "top_k", param_token)
                    else:
                        sae_class = VanillaSAE
                        active_key = (site_name, "sequence", "vanilla", param_token)

                    sae = sae_class(ckpt["cfg"])
                    sae.load_state_dict(ckpt["state_dict"])
                    sae.to(device).eval()

                    with torch.no_grad():
                        sae_out = sae(acts_tensor)
                        latent_acts = sae_out["feature_acts"].cpu().numpy()

                    activation_fracs = (np.abs(latent_acts) > args.epdf_activation_threshold).mean(axis=0)

                    filtered_latents: dict[str, list[float]] = {}
                    active_info = active_latents.get(active_key)
                    if isinstance(active_info, dict) and active_info:
                        for latent_id, stats in active_info.items():
                            try:
                                frac = float(stats[0])
                            except Exception:
                                frac = 0.0
                            try:
                                latent_idx_int = int(latent_id)
                            except Exception:
                                latent_idx_int = None
                            if latent_idx_int is not None and 0 <= latent_idx_int < activation_fracs.shape[0]:
                                frac = max(frac, float(activation_fracs[latent_idx_int]))
                            if frac >= args.epdf_latent_min_fraction:
                                filtered_latents[str(latent_id)] = [frac]
                    else:
                        for idx, frac in enumerate(activation_fracs):
                            if frac >= args.epdf_latent_min_fraction:
                                filtered_latents[str(idx)] = [float(frac)]

                    if not filtered_latents:
                        continue

                    # Apply max latents limit if specified (keep most active)
                    if args.max_latents_per_sae is not None and len(filtered_latents) > args.max_latents_per_sae:
                        # Sort by activity fraction (descending) and keep top N
                        sorted_latents = sorted(
                            filtered_latents.items(),
                            key=lambda x: x[1][0],
                            reverse=True
                        )
                        filtered_latents = dict(sorted_latents[:args.max_latents_per_sae])
                        print(
                            f"{site_name}/{sae_type}/{param_token}: "
                            f"limited to top {args.max_latents_per_sae} most active latents"
                        )

                    log_memory_usage(
                        f"Before EPDF build: {site_name}/{sae_type}/{param_token}",
                        args.verbose_memory
                    )

                    cache_path = None
                    if cache_dir is not None:
                        cache_name = f"{site_name}_{sae_type}_{param_token}.pkl"
                        cache_path = os.path.join(cache_dir, cache_name)

                    epdf_dict = None
                    if cache_path and args.epdf_use_cache and os.path.exists(cache_path):
                        try:
                            epdf_dict = load_latent_epdfs(cache_path)
                            print(f"Loaded EPDF cache from {cache_path}")
                        except Exception as exc:
                            print(f"Warning: failed to load EPDF cache {cache_path}: {exc}")
                            epdf_dict = None

                    if epdf_dict is None:
                        epdf_dict = build_mp_latent_epdfs(
                            site_name=site_name,
                            sae_id=(sae_type, param_token),
                            latent_activations=latent_acts,
                            component_beliefs=flattened_component_beliefs,
                            component_metadata=component_meta_map,
                            active_latent_indices=filtered_latents,
                            activation_threshold=args.epdf_activation_threshold,
                            bw_method=args.bandwidth,
                            progress=True,
                            progress_desc=f"Latents {site_name}/{sae_type}/{param_token}",
                            progress_disable=False, #not sys.stderr.isatty(),
                        )
                        if cache_path is not None:
                            try:
                                save_latent_epdfs(epdf_dict, cache_path)
                                print(f"Saved EPDF cache to {cache_path}")
                            except Exception as exc:
                                print(f"Warning: failed to save EPDF cache {cache_path}: {exc}")

                    if not epdf_dict:
                        continue

                    # Create organized directory structure
                    all_dir = os.path.join(epdf_output_dir, "all")
                    site_sae_dir = os.path.join(epdf_output_dir, site_name, param_token)
                    os.makedirs(all_dir, exist_ok=True)
                    os.makedirs(site_sae_dir, exist_ok=True)

                    sae_result_counter += 1
                    epdf_list = list(epdf_dict.values())
                    title_prefix = f"{site_name} / {sae_type} / {param_token}"

                    # Generate "all latents" figure with KDE visualization
                    if args.epdf_plot_mode in ["both", "all_only"]:
                        log_memory_usage(
                            f"Before all-latents plot: {site_name}/{sae_type}/{param_token}",
                            args.verbose_memory
                        )
                        fig_all = plot_mp_epdfs(
                            epdf_list,
                            component_order=component_order,
                            max_points=args.epdf_max_points,
                            title=f"{title_prefix}: all latents",
                            mode="transparency",
                            grid_size=100,
                        )
                        # Save to both "all" and site-specific directories
                        all_filename = f"{site_name}_{sae_type}_{param_token}_all.png"
                        fig_all.savefig(os.path.join(all_dir, all_filename), dpi=150, bbox_inches='tight')
                        fig_all.savefig(os.path.join(site_sae_dir, all_filename), dpi=150, bbox_inches='tight')
                        plt.close(fig_all)
                        log_memory_usage(
                            f"After all-latents plot: {site_name}/{sae_type}/{param_token}",
                            args.verbose_memory
                        )

                    # Generate per-latent figures
                    if args.epdf_plot_mode in ["both", "per_latent_only"]:
                        latent_items = list(epdf_dict.items())
                        for latent_idx, epdf in tqdm(
                            latent_items,
                            desc=f"Figures {site_name}/{sae_type}/{param_token}",
                            leave=False,
                            disable=not sys.stderr.isatty(),
                        ):
                            fig_latent = plot_mp_epdfs(
                                [epdf],
                                component_order=component_order,
                                max_points=args.epdf_max_points,
                                title=f"{title_prefix}: latent {latent_idx}",
                                mode="transparency",
                                grid_size=100,
                            )
                            # Save to site-specific directory
                            latent_filename = f"{site_name}_{sae_type}_{param_token}_latent_{latent_idx}.png"
                            fig_latent.savefig(os.path.join(site_sae_dir, latent_filename), dpi=150, bbox_inches='tight')
                            plt.close(fig_latent)

                    # Clear matplotlib state and force garbage collection after per-latent loop
                    plt.clf()
                    gc.collect()

                    # Clean up SAE-related objects to free memory
                    del sae, sae_out, latent_acts, epdf_dict, epdf_list
                    if device == "cuda" or (hasattr(torch.cuda, 'is_available') and torch.cuda.is_available()):
                        torch.cuda.empty_cache()
                    gc.collect()

                    log_memory_usage(
                        f"After SAE cleanup: {site_name}/{sae_type}/{param_token}",
                        args.verbose_memory
                    )

                # Cleanup after processing all SAEs for this site
                del acts_tensor
                if device == "cuda" or (hasattr(torch.cuda, 'is_available') and torch.cuda.is_available()):
                    torch.cuda.empty_cache()
                gc.collect()

                log_memory_usage(
                    f"After site cleanup: {site_name}",
                    args.verbose_memory
                )

            if sae_result_counter == 0:
                print("No EPDFs were generated; check thresholds and SAE availability")

#%%
# ==== 2D Simplex Plots of Mess3 Predicted Beliefs ===================================== #
# Project 3D belief vectors to the 2-simplex in 2D and color like PCA subplots.

# Choose positions already used for PCA/regression for consistency
prediction_positions = evaluation_positions

# Triangle border for plotting
tri_vertices = np.array([[0.0, 0.0], [1.0, 0.0], [0.5, float(np.sqrt(3)/2.0)]])
tri_x = np.r_[tri_vertices[:, 0], tri_vertices[0, 0]]
tri_y = np.r_[tri_vertices[:, 1], tri_vertices[0, 1]]
os.makedirs(args.fig_out_dir, exist_ok=True)
belief_out_dir = os.path.join(args.fig_out_dir, "beliefs" + "_" + args.linear_prediction_layer)
os.makedirs(os.path.join(belief_out_dir), exist_ok=True)

for idx, meta in enumerate(component_metadata):
    if meta["type"] != "mess3":
        continue
    comp_name = str(meta["name"])
    beliefs_np = np.asarray(component_belief_arrays[idx])  # (B, T, 3)
    obs_np = np.asarray(component_token_arrays[idx])       # (B, T)
    if beliefs_np.shape[-1] != 3:
        # Only handle 3-state Mess3 here
        continue

    for pos in prediction_positions:
        if pos < 0 or pos >= beliefs_np.shape[1]:
            continue
        metrics_pos = belief_regression_metrics.get(comp_name, {}).get("metrics", {}).get(pos)
        if not metrics_pos or "predictions" not in metrics_pos:
            continue

        y_true = np.asarray(metrics_pos["targets"])
        y_pred = np.asarray(metrics_pos["predictions"])

        xs_true, ys_true = project_simplex3_to_2d(y_true)
        xs_pred, ys_pred = project_simplex3_to_2d(y_pred)

        color_true_mix = np.clip(y_true @ MESS3_RGB, 0.0, 1.0)
        color_pred_mix = np.clip(y_pred @ MESS3_RGB, 0.0, 1.0)

        belief_plot_cache[comp_name][pos] = {
            "type": "mess3",
            "xs_true": xs_true.copy(),
            "ys_true": ys_true.copy(),
            "xs_pred": xs_pred.copy(),
            "ys_pred": ys_pred.copy(),
            "colors": color_true_mix.copy(),
        }

        rng = np.random.default_rng(0)
        sample_size = min(4000, xs_true.shape[0])
        sample_idx = rng.choice(xs_true.shape[0], size=sample_size, replace=False)

        fig, axes = plt.subplots(1, 2, figsize=(10.5, 5.2))
        for ax, xs, ys, cols, title in zip(
            axes,
            (xs_true[sample_idx], xs_pred[sample_idx]),
            (ys_true[sample_idx], ys_pred[sample_idx]),
            (color_true_mix[sample_idx], color_pred_mix[sample_idx]),
            ("Ground truth", "Linear probe prediction"),
        ):
            ax.plot(tri_x, tri_y, color="black", linewidth=1.0)
            ax.scatter(xs, ys, c=cols, s=10, alpha=0.8, edgecolors="none")
            ax.set_aspect('equal', adjustable='box')
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_title(title)

        fig.suptitle(f"Mess3 {comp_name}: belief simplex (pos {pos})", fontsize=13)
        fig.tight_layout()
        safe_name = "".join(c if c.isalnum() or c in ("_", "-") else "_" for c in comp_name)
        out_path = os.path.join(belief_out_dir, f"mess3_pred_simplex2d_{safe_name}_pos{pos}_beliefrgb.png")
        fig.savefig(out_path, dpi=160)
        plt.close(fig)
        print(f"Saved Mess3 simplex comparison (belief RGB) → {out_path}")

        # True-belief colors placed at predicted locations
        fig_lbl, axes_lbl = plt.subplots(1, 2, figsize=(10.5, 5.2))
        for ax, xs, ys, cols, title in zip(
            axes_lbl,
            (xs_true[sample_idx], xs_pred[sample_idx]),
            (ys_true[sample_idx], ys_pred[sample_idx]),
            (color_true_mix[sample_idx], color_true_mix[sample_idx]),
            ("Ground truth (belief colors)", "Prediction (true colors @ predicted pos)"),
        ):
            ax.plot(tri_x, tri_y, color="black", linewidth=1.0)
            ax.scatter(xs, ys, c=cols, s=10, alpha=0.8, edgecolors="none")
            ax.set_aspect('equal', adjustable='box')
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_title(title)

        fig_lbl.suptitle(f"Mess3 {comp_name}: belief simplex (pos {pos})", fontsize=13)
        fig_lbl.tight_layout()
        out_path_lbl = os.path.join(belief_out_dir, f"mess3_pred_simplex2d_{safe_name}_pos{pos}_truecolors.png")
        fig_lbl.savefig(out_path_lbl, dpi=160)
        plt.close(fig_lbl)
        print(f"Saved Mess3 simplex comparison (true colors @ predicted positions) → {out_path_lbl}")

#%%
# ==== 2D Prediction Plots for Tom Quantum (scatter P(s0) vs P(s1)) ===================== #

for idx, meta in enumerate(component_metadata):
    if meta["type"] != "tom_quantum":
        print(f"  {meta['name']} ({meta['type']}): skipping")
        continue
    comp_name = str(meta["name"])
    beliefs_np = np.asarray(component_belief_arrays[idx])  # (B, T, 3)
    obs_np = np.asarray(component_token_arrays[idx])       # (B, T)
    if beliefs_np.shape[-1] != 3:
        print(f"  {meta['name']} ({meta['type']}): skipping")
        continue

    for pos in evaluation_positions:
        if pos < 0 or pos >= beliefs_np.shape[1]:
            continue
        metrics_pos = belief_regression_metrics.get(comp_name, {}).get("metrics", {}).get(pos)
        if not metrics_pos or "predictions" not in metrics_pos:
            continue

        y_true_params = np.asarray(metrics_pos["targets"], dtype=np.float64)
        y_pred_params = np.asarray(metrics_pos["predictions"], dtype=np.float64)

        if idx >= len(ordered_components):
            print(f"  {comp_name}: missing component reference for emission coloring, skipping")
            continue

        process = ordered_components[idx].process
        vocab = getattr(process, "vocab_size", 0)
        if vocab <= 0:
            print(f"  {comp_name}: invalid vocab size {vocab}, skipping")
            continue

        obs_probs_true = np.array(
            jax.vmap(process.observation_probability_distribution)(
                jnp.asarray(y_true_params, dtype=jnp.float32)
            )
        )
        obs_probs_true = np.nan_to_num(obs_probs_true, nan=0.0, posinf=0.0, neginf=0.0)
        obs_true_sums = obs_probs_true.sum(axis=1, keepdims=True)
        valid_true = np.squeeze(obs_true_sums > 0, axis=-1)
        if valid_true.any():
            obs_probs_true[valid_true] /= obs_true_sums[valid_true, 0][:, None]
        if (~valid_true).any():
            obs_probs_true[~valid_true] = 1.0 / vocab

        color_true_mix = np.clip(obs_probs_true @ TOM_RGB, 0.0, 1.0)
        color_pred_mix = color_true_mix.copy()

        belief_plot_cache[comp_name][pos] = {
            "type": "tom_quantum",
            "xs_true": y_true_params[:, 1].copy(),
            "ys_true": y_true_params[:, 2].copy(),
            "xs_pred": y_pred_params[:, 1].copy(),
            "ys_pred": y_pred_params[:, 2].copy(),
            "colors": color_true_mix.copy(),
        }

        rng = np.random.default_rng(0)
        sample_size = min(4000, y_true_params.shape[0])
        sample_idx = rng.choice(y_true_params.shape[0], size=sample_size, replace=False)

        fig, axes = plt.subplots(1, 2, figsize=(10.5, 5.0))
        data_pairs = (
            (y_true_params[sample_idx, 1], y_true_params[sample_idx, 2], color_true_mix[sample_idx], "Ground truth"),
            (y_pred_params[sample_idx, 1], y_pred_params[sample_idx, 2], color_pred_mix[sample_idx], "Linear probe prediction"),
        )

        for ax, (xs, ys, cols, title) in zip(axes, data_pairs):
            ax.scatter(xs, ys, c=cols, s=14, alpha=0.85, edgecolors="none")
            ax.set_xlabel("Re coherence")
            ax.set_ylabel("Im coherence")
            ax.set_xlim(-0.35, 0.35)
            ax.set_ylim(-0.35, 0.35)
            ax.set_aspect('equal', adjustable='box')
            ax.set_title(title)
            ax.grid(True, linewidth=0.3, alpha=0.5)

        fig.suptitle(f"TomQ {comp_name}: coherence plane (pos {pos})", fontsize=13)
        fig.tight_layout()
        os.makedirs(args.fig_out_dir, exist_ok=True)
        safe_name = "".join(c if c.isalnum() or c in ("_", "-") else "_" for c in comp_name)
        out_path = os.path.join(belief_out_dir, f"tomq_coherence_{safe_name}_pos{pos}_emissionmix.png")
        fig.savefig(out_path, dpi=170)
        plt.close(fig)
        print(f"Saved Tom Quantum coherence plot (emission-weighted colors) → {out_path}")

        fig_lbl, axes_lbl = plt.subplots(1, 2, figsize=(10.5, 5.0))
        data_pairs_lbl = (
            (y_true_params[sample_idx, 1], y_true_params[sample_idx, 2], color_true_mix[sample_idx], "Ground truth (belief colors)",),
            (y_pred_params[sample_idx, 1], y_pred_params[sample_idx, 2], color_true_mix[sample_idx], "Prediction (true colors @ predicted pos)"),
        )

        for ax, (xs, ys, cols, title) in zip(axes_lbl, data_pairs_lbl):
            ax.scatter(xs, ys, c=cols, s=14, alpha=0.85, edgecolors="none")
            ax.set_xlabel("Re coherence")
            ax.set_ylabel("Im coherence")
            ax.set_xlim(-0.35, 0.35)
            ax.set_ylim(-0.35, 0.35)
            ax.set_aspect('equal', adjustable='box')
            ax.set_title(title)
            ax.grid(True, linewidth=0.3, alpha=0.5)

        fig_lbl.suptitle(f"TomQ {comp_name}: coherence plane (pos {pos})", fontsize=13)
        fig_lbl.tight_layout()
        out_path_lbl = os.path.join(belief_out_dir, f"tomq_coherence_{safe_name}_pos{pos}_truecolors.png")
        fig_lbl.savefig(out_path_lbl, dpi=170)
        plt.close(fig_lbl)
        print(f"Saved Tom Quantum coherence plot (true colors @ predicted positions) → {out_path_lbl}")



#%%
# ==== Composite Belief vs Prediction Grids (per position) ============================= #

if isinstance(data_source, MultipartiteSampler):
    component_order = [comp.name for comp in ordered_components]
else:
    component_order = [comp.name for comp in components]

component_type_map = {str(meta["name"]): str(meta["type"]) for meta in component_metadata}

for pos in evaluation_positions:
    fig, axes = plt.subplots(2, len(component_order), figsize=(3.5 * len(component_order), 6.5))
    if axes.ndim == 1:
        axes = axes.reshape(2, -1)

    for col, comp_name in enumerate(component_order):
        ax_true = axes[0, col]
        ax_pred = axes[1, col]
        cache_entry = belief_plot_cache.get(comp_name, {}).get(pos)

        if cache_entry is None:
            for ax in (ax_true, ax_pred):
                ax.text(0.5, 0.5, "No data", ha="center", va="center", fontsize=10)
                ax.axis("off")
            continue

        comp_type = cache_entry.get("type", component_type_map.get(comp_name, "unknown"))
        xs_true = np.asarray(cache_entry.get("xs_true"))
        ys_true = np.asarray(cache_entry.get("ys_true"))
        xs_pred = np.asarray(cache_entry.get("xs_pred"))
        ys_pred = np.asarray(cache_entry.get("ys_pred"))
        colors = np.asarray(cache_entry.get("colors"))

        coords_true = np.stack([xs_true, ys_true], axis=-1)
        coords_pred = np.stack([xs_pred, ys_pred], axis=-1)
        finite_mask = np.isfinite(coords_true).all(axis=1) & np.isfinite(coords_pred).all(axis=1)
        if colors.ndim == 2:
            finite_mask &= np.isfinite(colors).all(axis=1)
        else:
            finite_mask &= np.isfinite(colors)

        coords_true = coords_true[finite_mask]
        coords_pred = coords_pred[finite_mask]
        colors = colors[finite_mask]

        if coords_true.size == 0 or coords_pred.size == 0:
            for ax in (ax_true, ax_pred):
                ax.text(0.5, 0.5, "No finite data", ha="center", va="center", fontsize=10)
                ax.axis("off")
            continue

        if colors.ndim == 2:
            finite_colors = np.isfinite(colors).all(axis=1)
        else:
            finite_colors = np.isfinite(colors)

        finite_true = np.isfinite(coords_true).all(axis=1) & finite_colors
        finite_pred = np.isfinite(coords_pred).all(axis=1)
        common_indices = np.flatnonzero(finite_true & finite_pred)

        def choose_indices(indices: np.ndarray, max_points: int, seed: int) -> np.ndarray:
            if indices.size == 0:
                return indices
            if indices.size <= max_points:
                return indices
            rng_local = np.random.default_rng(seed)
            return rng_local.choice(indices, size=max_points, replace=False)

        pred_sel = choose_indices(common_indices, 4000, seed=0)
        coords_pred_s = coords_pred[pred_sel]
        colors_pred_s = colors[pred_sel]

        true_coords_display: np.ndarray | None = None
        true_colors_display: np.ndarray | None = None

        comp_obj = ordered_components[col] if isinstance(data_source, MultipartiteSampler) else components[col]
        try:
            rng_true = jax.random.PRNGKey(1931 + 97 * pos + col)
            sample_batch = 6000
            seq_needed = max(seq_len, pos + 2)
            keys = jax.random.split(rng_true, sample_batch)
            init_state = jnp.tile(comp_obj.process.initial_state, (sample_batch, 1))
            states, _ = comp_obj.process.generate(init_state, keys, seq_needed, True)
            if states.ndim == 2:
                states = states[:, None, :]
            states = np.asarray(states)[:, :-1, :]
            if pos >= states.shape[1]:
                raise IndexError("position exceeds sampled length")
            state_slice = states[:, pos, :]
            state_slice_np = np.asarray(state_slice)
            if comp_type == "mess3":
                xs_disp, ys_disp = project_simplex3_to_2d(state_slice_np)
                true_coords_display = np.stack([xs_disp, ys_disp], axis=-1)
                true_colors_display = np.clip(state_slice_np @ MESS3_RGB, 0.0, 1.0)
            elif comp_type == "tom_quantum":
                true_coords_display = state_slice_np[:, 1:3]
                obs_probs_true = np.array(
                    jax.vmap(comp_obj.process.observation_probability_distribution)(
                        jnp.asarray(state_slice_np, dtype=jnp.float32)
                    )
                )
                obs_probs_true = np.nan_to_num(obs_probs_true, nan=0.0, posinf=0.0, neginf=0.0)
                obs_sums = obs_probs_true.sum(axis=1, keepdims=True)
                valid_obs = obs_sums.squeeze(-1) > 0
                if valid_obs.any():
                    obs_probs_true[valid_obs] /= obs_sums[valid_obs, 0][:, None]
                if (~valid_obs).any():
                    obs_probs_true[~valid_obs] = 1.0 / TOM_RGB.shape[0]
                true_colors_display = np.clip(obs_probs_true @ TOM_RGB, 0.0, 1.0)
            else:
                true_coords_display = None
        except Exception as exc:  # pragma: no cover - diagnostic fallback
            print(f"    Warning: failed to resample true geometry for {comp_name} pos {pos}: {exc}")
            true_coords_display = None
            true_colors_display = None

        if true_coords_display is not None and true_coords_display.shape[0] > 4000:
            idx = choose_indices(np.arange(true_coords_display.shape[0]), 4000, seed=1)
            true_coords_display = true_coords_display[idx]
            true_colors_display = true_colors_display[idx]

        if comp_type == "mess3":
            point_size = 2.5
            panels = (
                (ax_true, true_coords_display, true_colors_display, f"{comp_name}\nTrue beliefs"),
                (ax_pred, coords_pred_s, colors_pred_s, "Predicted beliefs"),
            )
            for ax, coords, panel_colors, title in panels:
                ax.plot(tri_x, tri_y, color="black", linewidth=1.0)
                if coords is None or coords.shape[0] == 0:
                    ax.text(0.5, 0.5, "No data", ha="center", va="center", fontsize=10)
                    ax.axis("off")
                    continue
                ax.scatter(
                    coords[:, 0],
                    coords[:, 1],
                    c=panel_colors,
                    s=point_size,
                    alpha=0.5,
                    edgecolors="none",
                )
                ax.set_aspect('equal', adjustable='box')
                ax.set_xticks([])
                ax.set_yticks([])
                ax.set_title(title)

            if col == 0:
                ax_true.set_ylabel("True beliefs")
                ax_pred.set_ylabel("Predicted beliefs")

        elif comp_type == "tom_quantum":
            point_size = 2.5
            lim = 0.35
            panels = (
                (ax_true, true_coords_display, true_colors_display, f"{comp_name}\nTrue beliefs"),
                (ax_pred, coords_pred_s, colors_pred_s, "Predicted beliefs"),
            )
            for ax, coords, panel_colors, title in panels:
                if coords is None or coords.shape[0] == 0:
                    ax.text(0.5, 0.5, "No data", ha="center", va="center", fontsize=10)
                    ax.axis("off")
                    continue
                ax.scatter(
                    coords[:, 0],
                    coords[:, 1],
                    c=panel_colors,
                    s=point_size,
                    alpha=0.5,
                    edgecolors="none",
                )
                ax.set_xlim(-lim, lim)
                ax.set_ylim(-lim, lim)
                ax.set_aspect('equal', adjustable='box')
                ax.grid(True, linewidth=0.3, alpha=0.4)
                ax.set_title(title)
                if col == 0:
                    ax.set_xlabel("Re coherence")
                    ax.set_ylabel("Im coherence")
                else:
                    ax.set_xticks([])
                    ax.set_yticks([])
        else:
            for ax in (ax_true, ax_pred):
                ax.text(0.5, 0.5, f"Unsupported type\n{comp_type}", ha="center", va="center", fontsize=10)
                ax.axis('off')
            continue

    fig.suptitle(f"Belief vs predicted geometry (position {pos})", fontsize=15)
    fig.tight_layout(rect=(0, 0.02, 1, 0.95))
    os.makedirs(args.fig_out_dir, exist_ok=True)
    grid_out = os.path.join(belief_out_dir, f"belief_prediction_grid_pos{pos}.png")
    fig.savefig(grid_out, dpi=170)
    plt.close(fig)
    print(f"Saved composite belief/prediction grid → {grid_out}")


#%%
# ==== Ground-truth Belief Geometry Sweeps ============================================= #

os.makedirs(args.fig_out_dir, exist_ok=True)

mess3_x_grid = np.linspace(0.05, 0.3, 11)
mess3_a_grid = np.concatenate((np.linspace(0.05, 0.15, 5), np.linspace(0.20, 0.60, 5), np.linspace(0.65, 0.85, 5), np.linspace(0.875, 0.95, 4)))
mess3_grid_fig = plot_mess3_belief_grid(
    x_values=mess3_x_grid,
    a_values=mess3_a_grid,
    seq_position=14,
    batch_size=4096,
    seq_len=seq_len,
    seed=123,
    sample_size=2500,
)
mess3_grid_path = os.path.join(belief_out_dir, "mess3_belief_grid_pos9.png")
mess3_grid_fig.savefig(mess3_grid_path, dpi=170)
plt.close(mess3_grid_fig)
print(f"Saved Mess3 parameter grid → {mess3_grid_path}")

tomq_08013_fig = plot_tom_quantum_coherence(
    alpha=0.8,
    beta=1.3,
    seq_position=9,
    batch_size=4096,
    seq_len=seq_len,
    seed=432,
    sample_size=4000,
)
tomq_08013_path = os.path.join(belief_out_dir, "tomq_true_geometry_alpha0.8_beta1.3_pos9.png")
tomq_08013_fig.savefig(tomq_08013_path, dpi=180)
plt.close(tomq_08013_fig)
print(f"Saved TomQ true geometry (alpha=0.8, beta=1.3) → {tomq_08013_path}")

central_tomq_fig = plot_tom_quantum_coherence(
    alpha=1.0,
    beta=float(np.sqrt(51)),
    seq_position=9,
    batch_size=4096,
    seq_len=seq_len,
    seed=456,
    sample_size=4000,
)
tomq_central_path = os.path.join(belief_out_dir, "tomq_true_geometry_alpha1.0_beta_sqrt51_pos9.png")
central_tomq_fig.savefig(tomq_central_path, dpi=180)
plt.close(central_tomq_fig)
print(f"Saved TomQ true geometry (alpha=1.0, beta=sqrt(51)) → {tomq_central_path}")

alpha_center = 1.0
beta_center = float(np.sqrt(51))
tomq_alpha_grid = np.linspace(alpha_center - 0.12, alpha_center + 0.12, 5)
tomq_beta_grid = np.linspace(beta_center - 1.5, beta_center + 1.5, 5)
tomq_grid_sweep_fig = plot_tom_quantum_coherence_grid(
    alpha_values=tomq_alpha_grid,
    beta_values=tomq_beta_grid,
    seq_position=14,
    batch_size=4096,
    seq_len=seq_len,
    seed=789,
    sample_size=3500,
)
tomq_grid_sweep_path = os.path.join(belief_out_dir, "tomq_coherence_grid_pos9.png")
tomq_grid_sweep_fig.savefig(tomq_grid_sweep_path, dpi=170)
plt.close(tomq_grid_sweep_fig)
print(f"Saved TomQ parameter grid → {tomq_grid_sweep_path}")

wide_alpha_grid = np.concatenate(([0.25, 0.5, 0.75], np.linspace(1.0, 3.5, 6)))
wide_beta_grid = np.concatenate(([0.5], np.linspace(1, 8, 8), [10, 12]))
tomq_grid_wide_fig = plot_tom_quantum_coherence_grid(
    alpha_values=wide_alpha_grid,
    beta_values=wide_beta_grid,
    seq_position=14,
    batch_size=4096,
    seq_len=seq_len,
    seed=987,
    sample_size=3000,
)
tomq_grid_wide_path = os.path.join(belief_out_dir, "tomq_coherence_grid_wide_pos9.png")
tomq_grid_wide_fig.savefig(tomq_grid_wide_path, dpi=170)
plt.close(tomq_grid_wide_fig)
print(f"Saved TomQ wide parameter grid → {tomq_grid_wide_path}")


#%%
# ==== Plot PC Projections ============================================================= #
# ==== for All 9 choose 3 and 9 choose 2 combinations of the first 9 indices of PCA ==== #
##########################################################################################


# # Test. Don't delete
# test_inds = [1,2]
# if len(test_inds) == 2:
#     test_title = f"PCA PCs {test_inds[0]+1},{test_inds[1]+1}"
# elif len(test_inds) == 3:
#     test_title = f"3D PCA PCs {test_inds[0]+1},{test_inds[1]+1},{test_inds[2]+1}"
# else:
#     raise ValueError(f"Invalid number of indices: {len(test_inds)}")
# plot_pca_subplots(
#     pca_coords,
#     mess3_point_colors,
#     tom_point_colors,
#     pc_indices=test_inds,
#     marker_size=3,
#     opacity=0.6,
#     height=900,
#     width=900,
#     show=True,
#     title_text=test_title,
#     output_dir=args.fig_out_dir,
#     save=None,
#     mess3_label_to_color=mess3_label_to_color,
#     tom_label_to_color=tom_label_to_color,
#     n_points_to_plot=3000
# )
# n = 1

#%%
if plot_pcas:
    pca_indices = list(range(pca_coords.shape[1]))
    if len(pca_indices) >= 3:
        comb_3 = list(combinations(pca_indices, 3))
        for pc_inds in comb_3:
            plot_pca_subplots(
                pca_coords,
                mess3_point_colors,
                tom_point_colors,
                pc_indices=list(pc_inds),
                marker_size=2,
                opacity=0.6,
                height=900,
                width=900,
                show=False,
                title_text=f"3D PCA PCs {pc_inds[0]+1},{pc_inds[1]+1},{pc_inds[2]+1}",
                output_dir=os.path.join(args.fig_out_dir, "pca_plots_" + args.linear_prediction_layer),
                save=["html"],
                mess3_label_to_color=mess3_label_to_color,
                tom_label_to_color=tom_label_to_color
            )
    pca_indices = list(range(pca_coords.shape[1]))
    if len(pca_indices) >= 2:
        comb_2 = list(combinations(pca_indices, 2))
        for pc_inds in comb_2:
            plot_pca_subplots(
                pca_coords,
                mess3_point_colors,
                tom_point_colors,
                pc_indices=list(pc_inds),
                marker_size=3,
                opacity=0.6,
                height=700,
                width=700,
                show=False,
                title_text=f"2D PCA PCs {pc_inds[0]+1},{pc_inds[1]+1}",
                output_dir=os.path.join(args.fig_out_dir, "pca_plots_" + args.linear_prediction_layer),
                save=["html"],
                mess3_label_to_color=mess3_label_to_color,
                tom_label_to_color=tom_label_to_color,
                n_points_to_plot=3000
            )
        print(f"{len(pca_indices)} choose 2 =", comb(len(pca_indices), 2))
#%%
