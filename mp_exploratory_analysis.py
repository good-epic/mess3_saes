# -*- coding: utf-8 -*-
"""Simplexity-Mess3.ipynb

Automatically generated by Colab.

nOriginal file is located at
    https://colab.research.google.com/drive/1lo54AkaiN8cIygtVQC7lUNdbRpRFKQWy
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Installation (skip if already installed)
# %pip -q install --upgrade pip wheel setuptools
# %pip -q install "einops>=0.7.0" "jaxtyping>=0.2.28" "beartype>=0.14" better_abc
# %pip -q install --no-deps "transformer-lens>=2.16.1"
# %pip -q install "git+https://github.com/Astera-org/simplexity.git@MATS_2025_app"


#%%
# Cell 2: Setup
import argparse
import json
from copy import deepcopy
from itertools import combinations
from math import comb

import jax
import jax.numpy as jnp
import numpy as np
import torch
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from tqdm import tqdm
from transformer_lens import HookedTransformer, HookedTransformerConfig

from simplexity.generative_processes.torch_generator import generate_data_batch

from multipartite_utils import (
    MultipartiteSampler,
    build_components_from_config,
)
from training_and_analysis_utils import plot_pca_subplots

jax.config.update("jax_platform_name", "cpu")


PRESET_PROCESS_CONFIGS = {
    "single_mess3": [
        {"type": "mess3", "params": {"x": 0.1, "a": 0.7}},
    ],
    "3xmess3_2xtquant": [
        {
            "type": "mess3",
            "instances": [
                {"x": 0.1, "a": 0.8},
                {"x": 0.25, "a": 0.2},
                {"x": 0.4, "a": 0.5},
            ],
        },
        {
            "type": "tom_quantum",
            "instances": [
                {"alpha": 0.9, "beta": float(1.3)},
                {"alpha": 1.0, "beta": float(np.sqrt(51))},
            ],
        },
    ],
}



#%%
# CLI arguments
device_default = "cuda" if torch.cuda.is_available() else "cpu"
parser = argparse.ArgumentParser(description="Train Transformer on mess3 and SAEs")

# HookedTransformerConfig parameters
parser.add_argument("--d_model", type=int, default=128)
parser.add_argument("--n_heads", type=int, default=4)
parser.add_argument("--n_layers", type=int, default=3)
parser.add_argument("--n_ctx", type=int, default=16)
parser.add_argument("--d_vocab", type=int, default=None)
parser.add_argument("--act_fn", type=str, default="relu")
parser.add_argument("--device", type=str, default=device_default)
parser.add_argument("--d_head", type=int, default=32)

# SAE hyperparameters
parser.add_argument("--dict_mul", type=int, default=4)
parser.add_argument("--k", type=int, nargs="+", default=list(range(1, 25)))
#parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=[0.01, 0.015, 0.02, 0.025, 0.05, 0.075] + [round(x, 3) for x in np.arange(0.1, 0.151, 0.005)])
#parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=[1e-3, 0.01, 0.015, 0.02, 0.025, 0.05] + [round(x, 2) for x in np.arange(0.1, 0.651, 0.05)])
parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=None)
parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=None)
parser.add_argument("--input_unit_norm", dest="input_unit_norm", action="store_true", default=True)
parser.add_argument("--no_input_unit_norm", dest="input_unit_norm", action="store_false")
parser.add_argument("--n_batches_to_dead", type=int, default=5)
parser.add_argument("--top_k_aux", type=int, default=None)
parser.add_argument("--aux_penalty", type=float, default=1.0/32.0)
parser.add_argument("--bandwidth", type=float, default=0.001)

# SAE training loop controls
parser.add_argument("--sae_steps", type=int, default=10000)
parser.add_argument("--sae_batch_size", type=int, default=1024)
parser.add_argument("--sae_seq_len", type=int, default=None)
parser.add_argument(
    "--seq_len",
    type=int,
    default=None,
    help="Sequence length used for analysis/visualization batches; defaults to n_ctx",
)
parser.add_argument(
    "--analysis_batch_size",
    type=int,
    default=8192,
    help="Batch size for exploratory sampling from the multipartite stack",
)
parser.add_argument("--sae_output_dir", type=str, default="outputs/saes/multipartite_001", help="Directory to save trained SAEs and metrics")

# Model loading
#parser.add_argument("--load_model", type=str, default=None, help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--load_model", type=str, default="outputs/checkpoints/multipartite_001/checkpoint_step_500000_final.pt", help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--process_config", type=str, default=None, help="Path to JSON describing a stack of generative processes")
#parser.add_argument("--process_preset", type=str, default=None, help="Named preset for generative process configuration")
parser.add_argument("--process_preset", type=str, default="3xmess3_2xtquant", help="Named preset for generative process configuration")

# Parse known to be notebook-friendly
args, _ = parser.parse_known_args()

if args.process_config and args.process_preset:
    parser.error("Specify at most one of --process_config or --process_preset")

if args.process_config:
    with open(args.process_config, "r", encoding="utf-8") as f:
        process_config = json.load(f)
elif args.process_preset:
    if args.process_preset not in PRESET_PROCESS_CONFIGS:
        parser.error(f"Unknown process preset '{args.process_preset}'")
    process_config = deepcopy(PRESET_PROCESS_CONFIGS[args.process_preset])
else:
    process_config = deepcopy(PRESET_PROCESS_CONFIGS["single_mess3"])

components = build_components_from_config(process_config)
if len(components) == 1:
    data_source = components[0].process
    sampler: MultipartiteSampler | None = None
else:
    sampler = MultipartiteSampler(components)
    data_source = sampler

if isinstance(data_source, MultipartiteSampler):
    vocab_size = data_source.vocab_size
    num_states = data_source.belief_dim
    print(
        f"Multipartite process with components {[c.name for c in data_source.components]}"
        f" â†’ vocab_size={vocab_size}, belief_dim={num_states}"
    )
else:
    vocab_size = data_source.vocab_size
    num_states = data_source.num_states
    print(f"Process: vocab_size={vocab_size}, states={num_states}")

# Create TransformerLens model
device = args.device
cfg = HookedTransformerConfig(
    d_model=args.d_model,
    n_heads=args.n_heads,
    n_layers=args.n_layers,
    n_ctx=args.n_ctx,
    d_vocab=(args.d_vocab if args.d_vocab is not None else vocab_size),
    act_fn=args.act_fn,
    device=device,
    d_head=args.d_head,
)
model = HookedTransformer(cfg)
print(f"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}")

# Define a default sequence length for downstream analysis/visualization
seq_len = args.seq_len if args.seq_len is not None else cfg.n_ctx


#%%
## Load model

key = jax.random.PRNGKey(42)
# Cell 3: Train (or load)
losses = []
if args.load_model is None:
    raise ValueError("Must provide --load_model")

ckpt = torch.load(args.load_model, map_location=device, weights_only=False)
cfg_loaded = None
if isinstance(ckpt.get("config"), dict):
    cfg_loaded = HookedTransformerConfig.from_dict(ckpt["config"])
    model = HookedTransformer(cfg_loaded).to(device)
state_dict = ckpt.get("state_dict") or ckpt.get("model_state_dict")
if state_dict is None:
    raise KeyError("Checkpoint missing Transformer state dict")
if cfg_loaded is None:
    model = HookedTransformer(cfg).to(device)
model.load_state_dict(state_dict)  # type: ignore[arg-type]
#optimizer.load_state_dict(ckpt['optimizer_state_dict'])
losses = ckpt['losses']
model.eval()
print(f"Loaded model from {args.load_model}")


#%%
# ==== Generate PCA of Residual Stream ==== #
############################################
# Generate a batch for analysis using the multipartite sampler utilities

# ### Old Version

# n_tom_quantum = 2
# n_mess3 = 3
# tom_quantum_processes = [c.process for c in components[3:5]]
# mess3_processes = [c.process for c in components[0:3]]

# tom_stationaries = [tom_quantum_processes[i].initial_state for i in range(n_tom_quantum)]
# mess3_stationaries = [mess3_processes[i].initial_state for i in range(n_mess3)]
# tom_quantum_vocab_size = tom_quantum_processes[0].vocab_size
# mess3_vocab_size = mess3_processes[0].vocab_size
# product_vocab_size = (tom_quantum_vocab_size ** n_tom_quantum) * mess3_vocab_size ** n_mess3

# from training_and_analysis_utils import generate_mp_emissions
# key, tom_inputs_list, mess3_inputs_list, tokens = \
#     generate_mp_emissions(key,n_tom_quantum, n_mess3, tom_stationaries, mess3_stationaries,
#                             8192, seq_len, tom_quantum_processes, mess3_processes,
#                             tom_quantum_vocab_size, mess3_vocab_size, product_vocab_size, device)

# # Run with cache to get all activations
# logits, cache = model.run_with_cache(tokens)

# # Extract residual stream activations
# # Automatically extract all residual streams based on n_layers from command line arguments
# residual_streams = {'embeddings': cache['hook_embed']}
# for i in range(args.n_layers):
#     residual_streams[f'layer_{i}'] = cache[f'blocks.{i}.hook_resid_post']

# print("Activation shapes:")
# for name, acts in residual_streams.items():
#     print(f"  {name}: {acts.shape}")

# # Flatten for PCA
# token_inds = [5, 8, 11, 14]
# activations_flat = {}
# for name, acts in residual_streams.items():
#     acts_reshaped = acts[:,token_inds, :].reshape(-1, acts.shape[-1]).cpu().numpy()
#     activations_flat[name] = acts_reshaped

# print("Flattened activation shapes:")
# for name, acts in activations_flat.items():
#     print(f"  {name}: {acts.shape}")


# # Create labels for visualization
# print(f"{tom_inputs_list[0].shape=}")
# tom_labels_flat = []
# for i in range(len(tom_inputs_list)):
#     tom_labels_flat.append(tom_inputs_list[i][:,token_inds])
#     tom_labels_flat[-1] = tom_labels_flat[-1].reshape(-1)

# print("Flattened tom_inputs shapes:")
# for i in range(len(tom_labels_flat)):
#     print(f"  {i}: {tom_labels_flat[i].shape}")

# print(f"{mess3_inputs_list[0].shape=}")
# mess3_labels_flat = []
# for i in range(len(mess3_inputs_list)):
#     mess3_labels_flat.append(mess3_inputs_list[i][:,token_inds])
#     mess3_labels_flat[-1] = mess3_labels_flat[-1].reshape(-1)

# print("Flattened mess3_inputs shapes:")
# for i in range(len(mess3_labels_flat)):
#     print(f"  {i}: {mess3_labels_flat[i].shape}")

# print(f"\nTotal points for PCA: {activations_flat['layer_0'].shape[0]}")
# #print(f"Token distribution: {np.bincount(token_labels)}")


# # Perform PCA on the final layer activations (6 components for four 3D plots)
# #pca = PCA(n_components=15, whiten=True)
# pca = PCA(n_components=15, whiten=True)
# pca_coords = pca.fit_transform(activations_flat['layer_2'])

# print(f"PCA explained variance ratio: {pca.explained_variance_ratio_}")
# print(f"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}")

# #%%

# # Define categorical colors with maximal separation per set
# # Mess3: red, yellow, purple
# _mess3_base = ["#d62728", "#ffd92f", "#9467bd"]
# # Tom Quantum: dark blue, light blue, light green, dark green
# _tom_base = ["#1f77b4", "#aec7e8", "#98df8a", "#2ca02c"]

# mess3_colors = [_mess3_base[i % len(_mess3_base)] for i in range(mess3_vocab_size)]
# tom_colors = [_tom_base[i % len(_tom_base)] for i in range(tom_quantum_vocab_size)]

# # Create label-to-color dicts (hex strings)
# mess3_label_to_color = {int(label): mess3_colors[label] for label in range(mess3_vocab_size)}
# tom_label_to_color = {int(label): tom_colors[label] for label in range(tom_quantum_vocab_size)}

# # Map labels to colors
# mess3_point_colors = [[mess3_label_to_color[label] for label in mess3_labels] for mess3_labels in mess3_labels_flat]
# tom_point_colors = [[tom_label_to_color[label] for label in tom_labels] for tom_labels in tom_labels_flat]

# print(mess3_label_to_color)
# print(tom_label_to_color)

# # 1) lengths match
# assert pca_coords.shape[0] == len(mess3_point_colors[0]) == len(tom_point_colors[0])

# # 2) deterministic alignment spot-check: the first few rows should agree
# for arr in (mess3_point_colors + tom_point_colors):
#     assert isinstance(arr, list) and len(arr) == pca_coords.shape[0]

#%%



###
### Codex fucked version
# =========================
# Fixed "Codex" version
# =========================

analysis_batch_size = args.analysis_batch_size

# Work out component types in order
component_type_sequence: list[str] = []
for entry in process_config:
    t = entry["type"]
    if "instances" in entry:
        component_type_sequence.extend([t] * len(entry["instances"]))
    else:
        count = int(entry.get("count", 1))
        component_type_sequence.extend([t] * count)
if len(component_type_sequence) != len(components):
    # fall back to names if config & components mismatch
    component_type_sequence = [comp.name.split("_")[0] for comp in components]

# Sample one analysis batch
component_token_arrays: list[np.ndarray] = []
component_belief_arrays: list[np.ndarray] = []


if isinstance(data_source, MultipartiteSampler):
    key, belief_states, product_tokens, component_observations = data_source.sample(
        key, analysis_batch_size, seq_len
    )
    product_vocab_size = data_source.vocab_size
    tokens = torch.from_numpy(np.array(product_tokens)).long().to(device)
    beliefs_np = np.array(belief_states)
    component_token_arrays = [np.array(obs) for obs in component_observations]
    cursor = 0
    for dim in data_source.component_state_dims:
        component_belief_arrays.append(beliefs_np[..., cursor: cursor + dim])
        cursor += dim
    component_vocab_sizes = list(data_source.component_vocab_sizes)
else:
    raise TypeError("Data source is not a MultipartiteSampler")
    # key, subkey = jax.random.split(key)
    # init_states = jnp.repeat(data_source.initial_state[None, :], analysis_batch_size, axis=0)
    # beliefs_np, observations, _ = generate_data_batch(
    #     init_states, data_source, analysis_batch_size, seq_len, subkey
    # )
    # product_vocab_size = data_source.vocab_size
    # tokens = torch.from_numpy(np.array(observations)).long().to(device)
    # component_token_arrays = [np.array(observations)]
    # component_belief_arrays = [np.array(beliefs_np)]
    # component_vocab_sizes = [data_source.vocab_size]

# Component metadata (name/type/vocab/belief_dim)
component_metadata: list[dict[str, object]] = []
for idx, comp in enumerate(components):
    comp_type = component_type_sequence[idx] if idx < len(component_type_sequence) else comp.name.split("_")[0]
    vocab_size = component_vocab_sizes[idx] if idx < len(component_vocab_sizes) else getattr(comp.process, "vocab_size", product_vocab_size)
    belief_dim = int(component_belief_arrays[idx].shape[-1]) if idx < len(component_belief_arrays) else int(getattr(comp.process, "num_states", 0))
    component_metadata.append(
        {"name": comp.name, "type": comp_type, "vocab_size": int(vocab_size), "belief_dim": belief_dim}
    )

print("Analysis batch summary:")
print(f"  product tokens -> batch={tokens.shape[0]}, seq={tokens.shape[1]}, vocab={product_vocab_size}")
for meta, obs in zip(component_metadata, component_token_arrays):
    print(f"  {meta['name']} ({meta['type']}): tokens shape {obs.shape}, vocab={meta['vocab_size']}, belief_dim={meta['belief_dim']}")

# Run model & collect residual stream activations
logits, cache = model.run_with_cache(tokens)
residual_streams = {"embeddings": cache["hook_embed"]}
for i in range(args.n_layers):
    residual_streams[f"layer_{i}"] = cache[f"blocks.{i}.hook_resid_post"]

print("Activation shapes:")
for name, acts in residual_streams.items():
    print(f"  {name}: {acts.shape}")


# ---- PCA data prep (select specific token positions then flatten) ----
token_inds = [5, 8, 11, 14]  # positions to visualize
activations_flat: dict[str, np.ndarray] = {}
for name, acts in residual_streams.items():
    # acts: [batch, seq, d_model] -> take columns at token_inds -> [batch, len(token_inds), d_model]
    # then flatten to [batch*len(token_inds), d_model] in C-order (row-major)
    acts_sel = torch.Tensor(acts)[:, token_inds, :].reshape(-1, acts.shape[-1]).cpu().numpy()
    activations_flat[name] = acts_sel

print("Flattened activation shapes:")
for name, acts in activations_flat.items():
    print(f"  {name}: {acts.shape}")

# Choose which site to PCA
target_layer_name = f"layer_{args.n_layers - 1}" if args.n_layers > 0 else "embeddings"
if target_layer_name not in activations_flat:
    available_layers = ", ".join(sorted(activations_flat.keys()))
    raise KeyError(f"Requested layer '{target_layer_name}' not found. Available: {available_layers}")

target_acts = activations_flat[target_layer_name]
n_components = min(15, target_acts.shape[0], target_acts.shape[1])
if n_components < 2:
    raise ValueError(f"Not enough samples/features for PCA, got {n_components}.")

pca = PCA(n_components=n_components, whiten=True)
pca_coords = pca.fit_transform(target_acts)
print(f"PCA ({target_layer_name}) explained variance ratio: {pca.explained_variance_ratio_}")
print(f"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}")


# --- build labels with the sampler's true order preserved ---
tok_idx_t = torch.tensor(token_inds, device=device)

def flatten_bt_labels(obs_np: np.ndarray) -> np.ndarray:
    # obs_np: (B, T) ints from the sampler
    t = torch.from_numpy(np.asarray(obs_np)).to(device).long()   # (B, T)
    sel = t.index_select(1, tok_idx_t)                           # (B, K)
    return sel.contiguous().view(-1).cpu().numpy().astype(int)   # (B*K,)

# Pair observations with their metadata to preserve alignment
mess3_labels_flat: list[np.ndarray] = []
tom_labels_flat:   list[np.ndarray] = []
for meta, obs_np in zip(component_metadata, component_token_arrays):
    flat = flatten_bt_labels(obs_np)
    if meta["type"] == "mess3":
        mess3_labels_flat.append(flat)
    elif meta["type"] == "tom_quantum":
        tom_labels_flat.append(flat)

# palettes
_mess3_base = ["#d62728", "#ffd92f", "#9467bd"]
_tom_base   = ["#1f77b4", "#aec7e8", "#98df8a", "#2ca02c"]

mess3_vocab = max((int(m["vocab_size"]) for m in component_metadata if m["type"]=="mess3"), default=0)
tom_vocab   = max((int(m["vocab_size"]) for m in component_metadata if m["type"]=="tom_quantum"), default=0)
mess3_label_to_color   = {i: _mess3_base[i % len(_mess3_base)] for i in range(mess3_vocab)}
tom_label_to_color     = {i: _tom_base[i % len(_tom_base)]     for i in range(tom_vocab)}

# color arrays: one per component instance, each aligned 1:1 to pca_coords rows
mess3_point_colors = [[mess3_label_to_color[int(v)] for v in lab] for lab in mess3_labels_flat]
tom_point_colors   = [[tom_label_to_color[int(v)]   for v in lab] for lab in tom_labels_flat]

# sanity
N = pca_coords.shape[0]
for i, arr in enumerate(mess3_point_colors):
    assert len(arr) == N, f"mess3[{i}] color length {len(arr)} != {N}"
for i, arr in enumerate(tom_point_colors):
    assert len(arr) == N, f"tom[{i}] color length {len(arr)} != {N}"


#%%
cta = [c[:,token_inds] for c in component_token_arrays]
print(f"{product_tokens.shape=}")
print(f"{product_tokens[:6]=}")
print(f"{cta[0][:6]=}")
print(f"{mess3_labels_flat[0][:32]=}")
print(f"{mess3_point_colors[0][:16]=}")
print(f"{cta[1][:6]=}")
print(f"{mess3_labels_flat[1][:32]=}")
print(f"{mess3_point_colors[1][:16]=}")
print(f"{cta[2][:6]=}")
print(f"{mess3_labels_flat[2][:32]=}")
print(f"{mess3_point_colors[2][:16]=}")

#print(f"{type(residual_streams['layer_0'])=}")

# # ---- Build point-wise labels aligned to the SAME flatten order ----
# # We need one flat array per chosen component (not a list-of-arrays).
# component_names = [meta["name"] for meta in component_metadata]
# mess3_indices = [i for i, name in enumerate(component_names) if "mess3" in name]
# tom_quantum_indices = [i for i, name in enumerate(component_names) if "tom_quantum" in name]

# # Use list comprehension and flatten each observation
# mess3_obs = [torch.Tensor(component_observations[i])[:,token_inds].reshape(-1) for i in mess3_indices]
# tom_quantum_obs = [torch.Tensor(component_observations[i])[:,token_inds].reshape(-1) for i in tom_quantum_indices]
# print(f"mess3_obs lens={[x.shape for x in mess3_obs]}")
# print(f"tom_quantum_obs lens={[x.shape for x in tom_quantum_obs]}")

# # ---- Color maps ----
# _mess3_base = ["#d62728", "#ffd92f", "#9467bd"]
# _tom_base   = ["#1f77b4", "#aec7e8", "#98df8a", "#2ca02c"]

# mess3_vocab_candidates = [meta["vocab_size"] for meta in component_metadata if meta["type"] == "mess3"]
# tom_vocab_candidates   = [meta["vocab_size"] for meta in component_metadata if meta["type"] == "tom_quantum"]
# mess3_vocab_size = int(max(mess3_vocab_candidates)) if mess3_vocab_candidates else 0
# tom_vocab_size   = int(max(tom_vocab_candidates)) if tom_vocab_candidates else 0

# mess3_colors = [_mess3_base[i % len(_mess3_base)] for i in range(mess3_vocab_size)] if mess3_vocab_size else []
# tom_colors   = [_tom_base[i % len(_tom_base)]     for i in range(tom_vocab_size)]     if tom_vocab_size   else []

# mess3_label_to_color = {int(label): mess3_colors[label] for label in range(mess3_vocab_size)}
# tom_label_to_color = {int(label): tom_colors[label] for label in range(tom_vocab_size)}

# # Map labels to colors
# mess3_point_colors = [[mess3_label_to_color[label] for label in np.array(mess3_labels)] for mess3_labels in mess3_obs]
# tom_point_colors = [[tom_label_to_color[label] for label in np.array(tom_labels)] for tom_labels in tom_quantum_obs]


# # Sanity check: color arrays must match pca_coords rows
# assert (not mess3_point_colors) or (len(mess3_point_colors) == pca_coords.shape[0])
# assert (not tom_point_colors)   or (len(tom_point_colors)   == pca_coords.shape[0])

# # Optional: quick histogram to confirm label balance
# def _print_color_hist(label, arr):
#     if not arr:
#         return
#     vals, counts = np.unique(np.array(arr), return_counts=True)
#     print(label, dict(zip(vals, counts)))
# _print_color_hist("Mess3 colors", mess3_point_colors)
# _print_color_hist("TomQ colors",  tom_point_colors)


# N = pca_coords.shape[0]
# assert len(mess3_point_colors) in (0, N), (len(mess3_point_colors), N)
# assert len(tom_point_colors)   in (0, N), (len(tom_point_colors), N)



# Example usage for PCs 6-7 (i.e., indices 5,6):
# plot_pca_subplots(
#     pca_coords,
#     mess3_point_colors,
#     tom_point_colors,
#     pc_indices=[5,6],
#     title_text='2D PCA of Residual Stream (Layer 1)'
# )
#%%
# Test. Don't delete
test_inds = [1,2]
if len(test_inds) == 2:
    test_title = f"PCA PCs {test_inds[0]+1},{test_inds[1]+1}"
elif len(test_inds) == 3:
    test_title = f"3D PCA PCs {test_inds[0]+1},{test_inds[1]+1},{test_inds[2]+1}"
else:
    raise ValueError(f"Invalid number of indices: {len(test_inds)}")
plot_pca_subplots(
    pca_coords,
    mess3_point_colors,
    tom_point_colors,
    pc_indices=test_inds,
    marker_size=2,
    opacity=0.6,
    height=900,
    width=900,
    show=True,
    title_text=test_title,
    output_dir="outputs/reports/multipartite_001",
    save=None,
    mess3_label_to_color=mess3_label_to_color,
    tom_label_to_color=tom_label_to_color
)
n = 1


#%%
# ==== Plot PC Projections ============================================================= #
# ==== for All 9 choose 3 and 9 choose 2 combinations of the first 9 indices of PCA ==== #
##########################################################################################
pca_indices = list(range(pca_coords.shape[1]))
if len(pca_indices) >= 3:
    comb_3 = list(combinations(pca_indices, 3))
    for pc_inds in comb_3:
        plot_pca_subplots(
            pca_coords,
            mess3_point_colors,
            tom_point_colors,
            pc_indices=list(pc_inds),
            marker_size=2,
            opacity=0.6,
            height=900,
            width=900,
            show=False,
            title_text=f"3D PCA PCs {pc_inds[0]+1},{pc_inds[1]+1},{pc_inds[2]+1}",
            output_dir="outputs/reports/multipartite_001",
            save=["html"],
            mess3_label_to_color=mess3_label_to_color,
            tom_label_to_color=tom_label_to_color
        )
#%%
pca_indices = list(range(pca_coords.shape[1]))
if len(pca_indices) >= 2:
    comb_2 = list(combinations(pca_indices, 2))
    for pc_inds in comb_2:
        plot_pca_subplots(
            pca_coords,
            mess3_point_colors,
            tom_point_colors,
            pc_indices=list(pc_inds),
            marker_size=2,
            opacity=0.6,
            height=700,
            width=700,
            show=False,
            title_text=f"2D PCA PCs {pc_inds[0]+1},{pc_inds[1]+1}",
            output_dir="outputs/reports/multipartite_001",
            save=["html"],
            mess3_label_to_color=mess3_label_to_color,
            tom_label_to_color=tom_label_to_color
        )
    print(f"{len(pca_indices)} choose 2 =", comb(len(pca_indices), 2))
#%%
