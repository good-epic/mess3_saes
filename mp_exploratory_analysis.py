# -*- coding: utf-8 -*-
"""Simplexity-Mess3.ipynb

Automatically generated by Colab.

nOriginal file is located at
    https://colab.research.google.com/drive/1lo54AkaiN8cIygtVQC7lUNdbRpRFKQWy
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Installation (skip if already installed)
# %pip -q install --upgrade pip wheel setuptools
# %pip -q install "einops>=0.7.0" "jaxtyping>=0.2.28" "beartype>=0.14" better_abc
# %pip -q install --no-deps "transformer-lens>=2.16.1"
# %pip -q install "git+https://github.com/Astera-org/simplexity.git@MATS_2025_app"


#%%
# Cell 2: Setup
import argparse
import json
from copy import deepcopy
from collections import defaultdict
from itertools import combinations
from math import comb

import jax
import jax.numpy as jnp
import numpy as np
import torch
from sklearn.decomposition import PCA
from tqdm import tqdm
from transformer_lens import HookedTransformer, HookedTransformerConfig

from simplexity.generative_processes.torch_generator import generate_data_batch

from multipartite_utils import (
    MultipartiteSampler,
    build_components_from_config,
)
from training_and_analysis_utils import (
    plot_pca_subplots,
    evaluate_belief_state_linear_models,
    project_vectors_onto_simplex,
    enforce_tom_quantum_physicality,
    project_simplex3_to_2d,
    plot_mess3_belief_grid,
    plot_tom_quantum_coherence,
    plot_tom_quantum_coherence_grid,
)
import os
import matplotlib.pyplot as plt
from matplotlib import colors as mcolors

jax.config.update("jax_platform_name", "cpu")


PRESET_PROCESS_CONFIGS = {
    "single_mess3": [
        {"type": "mess3", "params": {"x": 0.1, "a": 0.7}},
    ],
    "3xmess3_2xtquant": [
        {
            "type": "tom_quantum",
            "instances": [
                #{"alpha": 0.8, "beta": float(1.3)},
                {"alpha": 1.0, "beta": float(np.sqrt(51))},
                {"alpha": 1.0, "beta": float(np.sqrt(51))},
            ],
        },
        {
            "type": "mess3",
            "instances": [
                {"x": 0.10, "a": 0.50},
                {"x": 0.25, "a": 0.80},
                {"x": 0.40, "a": 0.20},
            ],
        },
    ],
}
#%%
# CLI arguments
device_default = "cuda" if torch.cuda.is_available() else "cpu"
parser = argparse.ArgumentParser(description="Train Transformer on mess3 and SAEs", allow_abbrev=False)

# HookedTransformerConfig parameters
parser.add_argument("--d_model", type=int, default=128)
parser.add_argument("--n_heads", type=int, default=4)
parser.add_argument("--n_layers", type=int, default=3)
parser.add_argument("--n_ctx", type=int, default=16)
parser.add_argument("--d_vocab", type=int, default=None)
parser.add_argument("--act_fn", type=str, default="relu")
parser.add_argument("--device", type=str, default=device_default)
parser.add_argument("--d_head", type=int, default=32)

# SAE hyperparameters
parser.add_argument("--dict_mul", type=int, default=4)
parser.add_argument("--k", type=int, nargs="+", default=list(range(1, 25)))
#parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=[0.01, 0.015, 0.02, 0.025, 0.05, 0.075] + [round(x, 3) for x in np.arange(0.1, 0.151, 0.005)])
#parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=[1e-3, 0.01, 0.015, 0.02, 0.025, 0.05] + [round(x, 2) for x in np.arange(0.1, 0.651, 0.05)])
parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=None)
parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=None)
parser.add_argument("--input_unit_norm", dest="input_unit_norm", action="store_true", default=True)
parser.add_argument("--no_input_unit_norm", dest="input_unit_norm", action="store_false")
parser.add_argument("--n_batches_to_dead", type=int, default=5)
parser.add_argument("--top_k_aux", type=int, default=None)
parser.add_argument("--aux_penalty", type=float, default=1.0/32.0)
parser.add_argument("--bandwidth", type=float, default=0.001)

# SAE training loop controls
parser.add_argument("--sae_steps", type=int, default=10000)
parser.add_argument("--sae_batch_size", type=int, default=1024)
parser.add_argument("--sae_seq_len", type=int, default=None)
parser.add_argument("--seq_len", type=int, default=None, help="Sequence length used for analysis/visualization batches; defaults to n_ctx",)
parser.add_argument("--analysis_batch_size", type=int, default=8192, help="Batch size for exploratory sampling from the multipartite stack",)
parser.add_argument("--sae_output_dir", type=str, default="outputs/saes/multipartite_001", help="Directory to save trained SAEs and metrics")
parser.add_argument("--fig_out_dir", type=str, default="outputs/reports/multipartite_001", help="Directory to save matplotlib figures")


# Model loading
#parser.add_argument("--load_model", type=str, default=None, help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--load_model", type=str, default="outputs/checkpoints/multipartite_001/checkpoint_step_500000_final.pt", help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--process_config", type=str, default=None, help="Path to JSON describing a stack of generative processes")
#parser.add_argument("--process_preset", type=str, default=None, help="Named preset for generative process configuration")
parser.add_argument("--process_preset", type=str, default="3xmess3_2xtquant", help="Named preset for generative process configuration")

# Parse known to be notebook-friendly
args, _ = parser.parse_known_args()

if args.process_config and args.process_preset:
    parser.error("Specify at most one of --process_config or --process_preset")

if args.process_config:
    with open(args.process_config, "r", encoding="utf-8") as f:
        process_config = json.load(f)
elif args.process_preset:
    if args.process_preset not in PRESET_PROCESS_CONFIGS:
        parser.error(f"Unknown process preset '{args.process_preset}'")
    process_config = deepcopy(PRESET_PROCESS_CONFIGS[args.process_preset])
else:
    process_config = deepcopy(PRESET_PROCESS_CONFIGS["single_mess3"])

components = build_components_from_config(process_config)
if len(components) == 1:
    data_source = components[0].process
    sampler: MultipartiteSampler | None = None
else:
    sampler = MultipartiteSampler(components)
    data_source = sampler



if isinstance(data_source, MultipartiteSampler):
    vocab_size = data_source.vocab_size
    num_states = data_source.belief_dim
    print(
        f"Multipartite process with components {[c.name for c in data_source.components]}"
        f" → vocab_size={vocab_size}, belief_dim={num_states}"
    )
else:
    vocab_size = data_source.vocab_size
    num_states = data_source.num_states
    print(f"Process: vocab_size={vocab_size}, states={num_states}")

# Create TransformerLens model
device = args.device
cfg = HookedTransformerConfig(
    d_model=args.d_model,
    n_heads=args.n_heads,
    n_layers=args.n_layers,
    n_ctx=args.n_ctx,
    d_vocab=(args.d_vocab if args.d_vocab is not None else vocab_size),
    act_fn=args.act_fn,
    device=device,
    d_head=args.d_head,
)
model = HookedTransformer(cfg)
print(f"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}")

# Define a default sequence length for downstream analysis/visualization
seq_len = args.seq_len if args.seq_len is not None else cfg.n_ctx


#%%
## Load model

key = jax.random.PRNGKey(42)
# Cell 3: Train (or load)
losses = []
if args.load_model is None:
    raise ValueError("Must provide --load_model")

ckpt = torch.load(args.load_model, map_location=device, weights_only=False)
cfg_loaded = None
if isinstance(ckpt.get("config"), dict):
    cfg_loaded = HookedTransformerConfig.from_dict(ckpt["config"])
    model = HookedTransformer(cfg_loaded).to(device)
state_dict = ckpt.get("state_dict") or ckpt.get("model_state_dict")
if state_dict is None:
    raise KeyError("Checkpoint missing Transformer state dict")
if cfg_loaded is None:
    model = HookedTransformer(cfg).to(device)
model.load_state_dict(state_dict)  # type: ignore[arg-type]
#optimizer.load_state_dict(ckpt['optimizer_state_dict'])
losses = ckpt['losses']
model.eval()
print(f"Loaded model from {args.load_model}")


#%%
# ==== Plot Moving Average of Losses ==== #
#########################################
# Visualization of loss
# Calculate moving average
window_size = 50 # You can adjust the window size
moving_average = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')

# Plot original loss with transparency
plt.figure()
plt.plot(losses, alpha=0.5, label='Original Loss') 
# Plot moving average
plt.plot(range(window_size - 1, len(losses)), moving_average,
         label=f'Moving Average (window={window_size})') 

plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training Loss on Product Space (multiple processes)')

# Calculate ylim based on percentiles
ylim_min = 0.9999 * np.min(losses)
ylim_max = ylim_min + 0.25 * (np.max(losses) - ylim_min)
plt.ylim([ylim_min, ylim_max])
#plt.yscale('log')  # Set y-axis to logarithmic scale
# Add vertical dotted lines
plt.axvline(x=1000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=5000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=10000, color='gray', linestyle=':', label='10 dims')
plt.axvline(x=24000, color='gray', linestyle=':', label='10 dims')
plt.legend()
os.makedirs(args.fig_out_dir, exist_ok=True)
plt.tight_layout()
plt.savefig(os.path.join(args.fig_out_dir, 'loss_moving_average.png'))

plt.axvline(x=2000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=3000, color='gray', linestyle=':', label='6 dims')
plt.axvline(x=4000, color='gray', linestyle=':', label='6 dims')
plt.xlim(0,80000)
plt.savefig(os.path.join(args.fig_out_dir, 'loss_moving_average_xlim_100k.png'))
plt.close()


#%%
# ==== Generate PCA of Residual Stream ==== #
############################################
# Generate a batch for analysis using the multipartite sampler utilities

analysis_batch_size = args.analysis_batch_size

# Sample one analysis batch
component_token_arrays: list[np.ndarray] = []
component_belief_arrays: list[np.ndarray] = []


if isinstance(data_source, MultipartiteSampler):
    key, belief_states, product_tokens, component_observations = data_source.sample(
        key, analysis_batch_size, seq_len
    )
    product_vocab_size = data_source.vocab_size
    tokens = torch.from_numpy(np.array(product_tokens)).long().to(device)
    beliefs_np = np.array(belief_states)
    component_token_arrays = [np.array(obs) for obs in component_observations]
    cursor = 0
    for dim in data_source.component_state_dims:
        component_belief_arrays.append(beliefs_np[..., cursor: cursor + dim])
        cursor += dim
    component_vocab_sizes = list(data_source.component_vocab_sizes)
    ordered_components = list(data_source.components)
else:
    raise TypeError("Data source is not a MultipartiteSampler")
    # key, subkey = jax.random.split(key)
    # init_states = jnp.repeat(data_source.initial_state[None, :], analysis_batch_size, axis=0)
    # beliefs_np, observations, _ = generate_data_batch(
    #     init_states, data_source, analysis_batch_size, seq_len, subkey
    # )
    # product_vocab_size = data_source.vocab_size
    # tokens = torch.from_numpy(np.array(observations)).long().to(device)
    # component_token_arrays = [np.array(observations)]
    # component_belief_arrays = [np.array(beliefs_np)]
    # component_vocab_sizes = [data_source.vocab_size]

# Component metadata (name/type/vocab/belief_dim)
component_metadata: list[dict[str, object]] = []
if isinstance(data_source, MultipartiteSampler):
    component_iter = enumerate(ordered_components)
else:
    component_iter = enumerate(components)

for idx, comp in component_iter:
    comp_type = getattr(comp, "process_type", comp.name.split("_")[0])
    vocab_size = component_vocab_sizes[idx] if idx < len(component_vocab_sizes) else getattr(comp.process, "vocab_size", product_vocab_size)
    belief_dim = int(component_belief_arrays[idx].shape[-1]) if idx < len(component_belief_arrays) else int(getattr(comp.process, "num_states", 0))
    component_metadata.append(
        {"name": comp.name, "type": comp_type, "vocab_size": int(vocab_size), "belief_dim": belief_dim}
    )

print("Analysis batch summary:")
print(f"  product tokens -> batch={tokens.shape[0]}, seq={tokens.shape[1]}, vocab={product_vocab_size}")
for meta, obs in zip(component_metadata, component_token_arrays):
    print(f"  {meta['name']} ({meta['type']}): tokens shape {obs.shape}, vocab={meta['vocab_size']}, belief_dim={meta['belief_dim']}")

# Run model & collect residual stream activations
logits, cache = model.run_with_cache(tokens)
residual_streams = {"embeddings": cache["hook_embed"]}
for i in range(args.n_layers):
    residual_streams[f"layer_{i}"] = cache[f"blocks.{i}.hook_resid_post"]

print("Activation shapes:")
for name, acts in residual_streams.items():
    print(f"  {name}: {acts.shape}")


# ---- PCA data prep (select specific token positions then flatten) ----
token_inds = [4, 9, 14]  # positions (0-indexed) to visualize / probe
activations_flat: dict[str, np.ndarray] = {}
for name, acts in residual_streams.items():
    # acts: [batch, seq, d_model] -> take columns at token_inds -> [batch, len(token_inds), d_model]
    # then flatten to [batch*len(token_inds), d_model] in C-order (row-major)
    acts_sel = torch.Tensor(acts)[:, token_inds, :].reshape(-1, acts.shape[-1]).cpu().numpy()
    activations_flat[name] = acts_sel

print("Flattened activation shapes:")
for name, acts in activations_flat.items():
    print(f"  {name}: {acts.shape}")

# Choose which site to PCA
target_layer_name = f"layer_{args.n_layers - 1}" if args.n_layers > 0 else "embeddings"
if target_layer_name not in activations_flat:
    available_layers = ", ".join(sorted(activations_flat.keys()))
    raise KeyError(f"Requested layer '{target_layer_name}' not found. Available: {available_layers}")

target_acts = activations_flat[target_layer_name]
n_components = min(15, target_acts.shape[0], target_acts.shape[1])
if n_components < 2:
    raise ValueError(f"Not enough samples/features for PCA, got {n_components}.")

pca = PCA(n_components=n_components, whiten=True)
pca_coords = pca.fit_transform(target_acts)
print(f"PCA ({target_layer_name}) explained variance ratio: {pca.explained_variance_ratio_}")
print(f"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}")


# --- build labels with the sampler's true order preserved ---
tok_idx_t = torch.tensor(token_inds, device=device)

def flatten_bt_labels(obs_np: np.ndarray) -> np.ndarray:
    # obs_np: (B, T) ints from the sampler
    t = torch.from_numpy(np.asarray(obs_np)).to(device).long()   # (B, T)
    sel = t.index_select(1, tok_idx_t)                           # (B, K)
    return sel.contiguous().view(-1).cpu().numpy().astype(int)   # (B*K,)

# Pair observations with their metadata to preserve alignment
mess3_label_entries: list[tuple[str, np.ndarray]] = []
tom_label_entries: list[tuple[str, np.ndarray]] = []
for meta, obs_np in zip(component_metadata, component_token_arrays):
    flat = flatten_bt_labels(obs_np)
    if meta["type"] == "mess3":
        mess3_label_entries.append((meta["name"], flat))
    elif meta["type"] == "tom_quantum":
        tom_label_entries.append((meta["name"], flat))

# palettes
_mess3_base = ["#ff595e", "#1982c4", "#8ac926"]
_tom_base   = ["#ff1744", "#00c853", "#2962ff", "#ffd600"]
MESS3_RGB = np.array([mcolors.to_rgb(hex_code) for hex_code in _mess3_base], dtype=np.float64)
TOM_RGB = np.array([mcolors.to_rgb(hex_code) for hex_code in ["#ff1744", "#00c853", "#2962ff", "#ffd600"]], dtype=np.float64)

mess3_vocab = max((int(m["vocab_size"]) for m in component_metadata if m["type"]=="mess3"), default=0)
tom_vocab   = max((int(m["vocab_size"]) for m in component_metadata if m["type"]=="tom_quantum"), default=0)
mess3_label_to_color   = {i: _mess3_base[i % len(_mess3_base)] for i in range(mess3_vocab)}
tom_label_to_color     = {i: _tom_base[i % len(_tom_base)]     for i in range(tom_vocab)}

# Derive deterministic ordering consistent with the original configuration
mess3_order = [comp.name for comp in components if getattr(comp, "process_type", comp.name.split("_")[0]) == "mess3"]
tom_order   = [comp.name for comp in components if getattr(comp, "process_type", comp.name.split("_")[0]) == "tom_quantum"]

mess3_label_map = {name: lab for name, lab in mess3_label_entries}
tom_label_map   = {name: lab for name, lab in tom_label_entries}

mess3_point_colors = [
    [mess3_label_to_color[int(v)] for v in mess3_label_map[name]]
    for name in mess3_order
]
tom_point_colors = [
    [tom_label_to_color[int(v)] for v in tom_label_map[name]]
    for name in tom_order
]

# keep flattened label arrays for downstream checks if needed
mess3_labels_flat = [mess3_label_map[name] for name in mess3_order]
tom_labels_flat   = [tom_label_map[name]   for name in tom_order]

# Cache per-component belief/prediction geometry for later composite figures
belief_plot_cache: dict[str, dict[int, dict[str, np.ndarray]]] = defaultdict(dict)

# sanity
N = pca_coords.shape[0]
for i, arr in enumerate(mess3_point_colors):
    assert len(arr) == N, f"mess3[{i}] color length {len(arr)} != {N}"
for i, arr in enumerate(tom_point_colors):
    assert len(arr) == N, f"tom[{i}] color length {len(arr)} != {N}"



# ==== Belief-State Linear Probing ============================================ #

# Use the same positions as PCA sampling to keep comparisons consistent
evaluation_positions = token_inds
final_layer_name = f"layer_{args.n_layers - 1}"
if final_layer_name not in residual_streams:
    available = ", ".join(sorted(residual_streams.keys()))
    raise KeyError(f"Expected residual stream '{final_layer_name}' not found. Have: {available}")

final_layer_activations = residual_streams[final_layer_name]

belief_regression_metrics = evaluate_belief_state_linear_models(
    activations=final_layer_activations,
    component_belief_arrays=component_belief_arrays,
    component_metadata=component_metadata,
    seq_positions=evaluation_positions,
    skip_dims_by_type={"tom_quantum": [0]},
    postprocess_by_type={
        "mess3": lambda arr: project_vectors_onto_simplex(arr, axis=-1),
        # Intentionally skip Tom Quantum clamping so we can inspect raw predictions.
    },
    store_predictions=True,
)

print("\n=== Linear Regression: belief prediction at final layer ===")
for comp_name, info in belief_regression_metrics.items():
    comp_type = info.get("type", "unknown")
    belief_dim = info.get("belief_dim")
    print(f"  {comp_name} [{comp_type}] belief_dim={belief_dim}")
    metrics_by_pos = info.get("metrics", {})
    for pos in evaluation_positions:
        metrics = metrics_by_pos.get(pos)
        if metrics is None:
            print(f"    pos {pos}: metrics not available")
            continue
        target_dims = metrics.get("target_dims", [])
        if not target_dims:
            note = metrics.get("note", "no usable belief dims")
            dropped_explicit = metrics.get("explicitly_dropped_dims", [])
            dropped_auto = metrics.get("dropped_low_variance_dims", [])
            print(
                f"    pos {pos}: {note} (explicitly dropped={dropped_explicit}, auto-dropped={dropped_auto})"
            )
            continue

        r2_mean = metrics.get("r2_mean", float("nan"))
        rmse = metrics.get("rmse", float("nan"))
        mae = metrics.get("mae", float("nan"))
        per_dim_r2 = metrics.get("r2_per_dim", {})
        per_dim_corr = metrics.get("pearson_per_dim", {})

        dims_repr = ",".join(str(d) for d in target_dims)
        r2_repr = ", ".join(f"{dim}:{per_dim_r2.get(dim, float('nan')):.3f}" for dim in target_dims)
        corr_repr = ", ".join(f"{dim}:{per_dim_corr.get(dim, float('nan')):.3f}" for dim in target_dims)

        print(
            f"    pos {pos}: dims[{dims_repr}] r2_mean={r2_mean:.3f} rmse={rmse:.5f} mae={mae:.5f}"
        )
        print(f"        r2_per_dim: {r2_repr}")
        print(f"        pearson_per_dim: {corr_repr}")

#%%
# ==== 2D Simplex Plots of Mess3 Predicted Beliefs ===================================== #
# Project 3D belief vectors to the 2-simplex in 2D and color like PCA subplots.

# Choose positions already used for PCA/regression for consistency
prediction_positions = evaluation_positions

# Triangle border for plotting
tri_vertices = np.array([[0.0, 0.0], [1.0, 0.0], [0.5, float(np.sqrt(3)/2.0)]])
tri_x = np.r_[tri_vertices[:, 0], tri_vertices[0, 0]]
tri_y = np.r_[tri_vertices[:, 1], tri_vertices[0, 1]]

for idx, meta in enumerate(component_metadata):
    if meta["type"] != "mess3":
        continue
    comp_name = str(meta["name"])
    beliefs_np = np.asarray(component_belief_arrays[idx])  # (B, T, 3)
    obs_np = np.asarray(component_token_arrays[idx])       # (B, T)
    if beliefs_np.shape[-1] != 3:
        # Only handle 3-state Mess3 here
        continue

    for pos in prediction_positions:
        if pos < 0 or pos >= beliefs_np.shape[1]:
            continue
        metrics_pos = belief_regression_metrics.get(comp_name, {}).get("metrics", {}).get(pos)
        if not metrics_pos or "predictions" not in metrics_pos:
            continue

        y_true = np.asarray(metrics_pos["targets"])
        y_pred = np.asarray(metrics_pos["predictions"])

        xs_true, ys_true = project_simplex3_to_2d(y_true)
        xs_pred, ys_pred = project_simplex3_to_2d(y_pred)

        color_true_mix = np.clip(y_true @ MESS3_RGB, 0.0, 1.0)
        color_pred_mix = np.clip(y_pred @ MESS3_RGB, 0.0, 1.0)

        belief_plot_cache[comp_name][pos] = {
            "type": "mess3",
            "xs_true": xs_true.copy(),
            "ys_true": ys_true.copy(),
            "xs_pred": xs_pred.copy(),
            "ys_pred": ys_pred.copy(),
            "colors": color_true_mix.copy(),
        }

        rng = np.random.default_rng(0)
        sample_size = min(4000, xs_true.shape[0])
        sample_idx = rng.choice(xs_true.shape[0], size=sample_size, replace=False)

        fig, axes = plt.subplots(1, 2, figsize=(10.5, 5.2))
        for ax, xs, ys, cols, title in zip(
            axes,
            (xs_true[sample_idx], xs_pred[sample_idx]),
            (ys_true[sample_idx], ys_pred[sample_idx]),
            (color_true_mix[sample_idx], color_pred_mix[sample_idx]),
            ("Ground truth", "Linear probe prediction"),
        ):
            ax.plot(tri_x, tri_y, color="black", linewidth=1.0)
            ax.scatter(xs, ys, c=cols, s=10, alpha=0.8, edgecolors="none")
            ax.set_aspect('equal', adjustable='box')
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_title(title)

        fig.suptitle(f"Mess3 {comp_name}: belief simplex (pos {pos})", fontsize=13)
        fig.tight_layout()
        os.makedirs(args.fig_out_dir, exist_ok=True)
        safe_name = "".join(c if c.isalnum() or c in ("_", "-") else "_" for c in comp_name)
        out_path = os.path.join(args.fig_out_dir, f"mess3_pred_simplex2d_{safe_name}_pos{pos}_beliefrgb.png")
        fig.savefig(out_path, dpi=160)
        plt.close(fig)
        print(f"Saved Mess3 simplex comparison (belief RGB) → {out_path}")

        # True-belief colors placed at predicted locations
        fig_lbl, axes_lbl = plt.subplots(1, 2, figsize=(10.5, 5.2))
        for ax, xs, ys, cols, title in zip(
            axes_lbl,
            (xs_true[sample_idx], xs_pred[sample_idx]),
            (ys_true[sample_idx], ys_pred[sample_idx]),
            (color_true_mix[sample_idx], color_true_mix[sample_idx]),
            ("Ground truth (belief colors)", "Prediction (true colors @ predicted pos)"),
        ):
            ax.plot(tri_x, tri_y, color="black", linewidth=1.0)
            ax.scatter(xs, ys, c=cols, s=10, alpha=0.8, edgecolors="none")
            ax.set_aspect('equal', adjustable='box')
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_title(title)

        fig_lbl.suptitle(f"Mess3 {comp_name}: belief simplex (pos {pos})", fontsize=13)
        fig_lbl.tight_layout()
        out_path_lbl = os.path.join(args.fig_out_dir, f"mess3_pred_simplex2d_{safe_name}_pos{pos}_truecolors.png")
        fig_lbl.savefig(out_path_lbl, dpi=160)
        plt.close(fig_lbl)
        print(f"Saved Mess3 simplex comparison (true colors @ predicted positions) → {out_path_lbl}")

#%%
# ==== 2D Prediction Plots for Tom Quantum (scatter P(s0) vs P(s1)) ===================== #

for idx, meta in enumerate(component_metadata):
    if meta["type"] != "tom_quantum":
        print(f"  {meta['name']} ({meta['type']}): skipping")
        continue
    comp_name = str(meta["name"])
    beliefs_np = np.asarray(component_belief_arrays[idx])  # (B, T, 3)
    obs_np = np.asarray(component_token_arrays[idx])       # (B, T)
    if beliefs_np.shape[-1] != 3:
        print(f"  {meta['name']} ({meta['type']}): skipping")
        continue

    for pos in evaluation_positions:
        if pos < 0 or pos >= beliefs_np.shape[1]:
            continue
        metrics_pos = belief_regression_metrics.get(comp_name, {}).get("metrics", {}).get(pos)
        if not metrics_pos or "predictions" not in metrics_pos:
            continue

        y_true_params = np.asarray(metrics_pos["targets"], dtype=np.float64)
        y_pred_params = np.asarray(metrics_pos["predictions"], dtype=np.float64)

        if idx >= len(ordered_components):
            print(f"  {comp_name}: missing component reference for emission coloring, skipping")
            continue

        process = ordered_components[idx].process
        vocab = getattr(process, "vocab_size", 0)
        if vocab <= 0:
            print(f"  {comp_name}: invalid vocab size {vocab}, skipping")
            continue

        obs_probs_true = np.array(
            jax.vmap(process.observation_probability_distribution)(
                jnp.asarray(y_true_params, dtype=jnp.float32)
            )
        )
        obs_probs_true = np.nan_to_num(obs_probs_true, nan=0.0, posinf=0.0, neginf=0.0)
        obs_true_sums = obs_probs_true.sum(axis=1, keepdims=True)
        valid_true = np.squeeze(obs_true_sums > 0, axis=-1)
        if valid_true.any():
            obs_probs_true[valid_true] /= obs_true_sums[valid_true, 0][:, None]
        if (~valid_true).any():
            obs_probs_true[~valid_true] = 1.0 / vocab

        color_true_mix = np.clip(obs_probs_true @ TOM_RGB, 0.0, 1.0)
        color_pred_mix = color_true_mix.copy()

        belief_plot_cache[comp_name][pos] = {
            "type": "tom_quantum",
            "xs_true": y_true_params[:, 1].copy(),
            "ys_true": y_true_params[:, 2].copy(),
            "xs_pred": y_pred_params[:, 1].copy(),
            "ys_pred": y_pred_params[:, 2].copy(),
            "colors": color_true_mix.copy(),
        }

        rng = np.random.default_rng(0)
        sample_size = min(4000, y_true_params.shape[0])
        sample_idx = rng.choice(y_true_params.shape[0], size=sample_size, replace=False)

        fig, axes = plt.subplots(1, 2, figsize=(10.5, 5.0))
        data_pairs = (
            (y_true_params[sample_idx, 1], y_true_params[sample_idx, 2], color_true_mix[sample_idx], "Ground truth"),
            (y_pred_params[sample_idx, 1], y_pred_params[sample_idx, 2], color_pred_mix[sample_idx], "Linear probe prediction"),
        )

        for ax, (xs, ys, cols, title) in zip(axes, data_pairs):
            ax.scatter(xs, ys, c=cols, s=14, alpha=0.85, edgecolors="none")
            ax.set_xlabel("Re coherence")
            ax.set_ylabel("Im coherence")
            ax.set_xlim(-0.35, 0.35)
            ax.set_ylim(-0.35, 0.35)
            ax.set_aspect('equal', adjustable='box')
            ax.set_title(title)
            ax.grid(True, linewidth=0.3, alpha=0.5)

        fig.suptitle(f"TomQ {comp_name}: coherence plane (pos {pos})", fontsize=13)
        fig.tight_layout()
        os.makedirs(args.fig_out_dir, exist_ok=True)
        safe_name = "".join(c if c.isalnum() or c in ("_", "-") else "_" for c in comp_name)
        out_path = os.path.join(args.fig_out_dir, f"tomq_coherence_{safe_name}_pos{pos}_emissionmix.png")
        fig.savefig(out_path, dpi=170)
        plt.close(fig)
        print(f"Saved Tom Quantum coherence plot (emission-weighted colors) → {out_path}")

        fig_lbl, axes_lbl = plt.subplots(1, 2, figsize=(10.5, 5.0))
        data_pairs_lbl = (
            (y_true_params[sample_idx, 1], y_true_params[sample_idx, 2], color_true_mix[sample_idx], "Ground truth (belief colors)",),
            (y_pred_params[sample_idx, 1], y_pred_params[sample_idx, 2], color_true_mix[sample_idx], "Prediction (true colors @ predicted pos)"),
        )

        for ax, (xs, ys, cols, title) in zip(axes_lbl, data_pairs_lbl):
            ax.scatter(xs, ys, c=cols, s=14, alpha=0.85, edgecolors="none")
            ax.set_xlabel("Re coherence")
            ax.set_ylabel("Im coherence")
            ax.set_xlim(-0.35, 0.35)
            ax.set_ylim(-0.35, 0.35)
            ax.set_aspect('equal', adjustable='box')
            ax.set_title(title)
            ax.grid(True, linewidth=0.3, alpha=0.5)

        fig_lbl.suptitle(f"TomQ {comp_name}: coherence plane (pos {pos})", fontsize=13)
        fig_lbl.tight_layout()
        out_path_lbl = os.path.join(args.fig_out_dir, f"tomq_coherence_{safe_name}_pos{pos}_truecolors.png")
        fig_lbl.savefig(out_path_lbl, dpi=170)
        plt.close(fig_lbl)
        print(f"Saved Tom Quantum coherence plot (true colors @ predicted positions) → {out_path_lbl}")



#%%
# ==== Composite Belief vs Prediction Grids (per position) ============================= #

if isinstance(data_source, MultipartiteSampler):
    component_order = [comp.name for comp in ordered_components]
else:
    component_order = [comp.name for comp in components]

component_type_map = {str(meta["name"]): str(meta["type"]) for meta in component_metadata}

for pos in evaluation_positions:
    fig, axes = plt.subplots(2, len(component_order), figsize=(3.5 * len(component_order), 6.5))
    if axes.ndim == 1:
        axes = axes.reshape(2, -1)

    for col, comp_name in enumerate(component_order):
        ax_true = axes[0, col]
        ax_pred = axes[1, col]
        cache_entry = belief_plot_cache.get(comp_name, {}).get(pos)

        if cache_entry is None:
            for ax in (ax_true, ax_pred):
                ax.text(0.5, 0.5, "No data", ha="center", va="center", fontsize=10)
                ax.axis("off")
            continue

        comp_type = cache_entry.get("type", component_type_map.get(comp_name, "unknown"))
        xs_true = np.asarray(cache_entry.get("xs_true"))
        ys_true = np.asarray(cache_entry.get("ys_true"))
        xs_pred = np.asarray(cache_entry.get("xs_pred"))
        ys_pred = np.asarray(cache_entry.get("ys_pred"))
        colors = np.asarray(cache_entry.get("colors"))

        coords_true = np.stack([xs_true, ys_true], axis=-1)
        coords_pred = np.stack([xs_pred, ys_pred], axis=-1)
        finite_mask = np.isfinite(coords_true).all(axis=1) & np.isfinite(coords_pred).all(axis=1)
        if colors.ndim == 2:
            finite_mask &= np.isfinite(colors).all(axis=1)
        else:
            finite_mask &= np.isfinite(colors)

        coords_true = coords_true[finite_mask]
        coords_pred = coords_pred[finite_mask]
        colors = colors[finite_mask]

        if coords_true.size == 0 or coords_pred.size == 0:
            for ax in (ax_true, ax_pred):
                ax.text(0.5, 0.5, "No finite data", ha="center", va="center", fontsize=10)
                ax.axis("off")
            continue

        if colors.ndim == 2:
            finite_colors = np.isfinite(colors).all(axis=1)
        else:
            finite_colors = np.isfinite(colors)

        finite_true = np.isfinite(coords_true).all(axis=1) & finite_colors
        finite_pred = np.isfinite(coords_pred).all(axis=1)
        common_indices = np.flatnonzero(finite_true & finite_pred)

        def choose_indices(indices: np.ndarray, max_points: int, seed: int) -> np.ndarray:
            if indices.size == 0:
                return indices
            if indices.size <= max_points:
                return indices
            rng_local = np.random.default_rng(seed)
            return rng_local.choice(indices, size=max_points, replace=False)

        pred_sel = choose_indices(common_indices, 4000, seed=0)
        coords_pred_s = coords_pred[pred_sel]
        colors_pred_s = colors[pred_sel]

        true_coords_display: np.ndarray | None = None
        true_colors_display: np.ndarray | None = None

        comp_obj = ordered_components[col] if isinstance(data_source, MultipartiteSampler) else components[col]
        try:
            rng_true = jax.random.PRNGKey(1931 + 97 * pos + col)
            sample_batch = 6000
            seq_needed = max(seq_len, pos + 2)
            keys = jax.random.split(rng_true, sample_batch)
            init_state = jnp.tile(comp_obj.process.initial_state, (sample_batch, 1))
            states, _ = comp_obj.process.generate(init_state, keys, seq_needed, True)
            if states.ndim == 2:
                states = states[:, None, :]
            states = np.asarray(states)[:, :-1, :]
            if pos >= states.shape[1]:
                raise IndexError("position exceeds sampled length")
            state_slice = states[:, pos, :]
            state_slice_np = np.asarray(state_slice)
            if comp_type == "mess3":
                xs_disp, ys_disp = project_simplex3_to_2d(state_slice_np)
                true_coords_display = np.stack([xs_disp, ys_disp], axis=-1)
                true_colors_display = np.clip(state_slice_np @ MESS3_RGB, 0.0, 1.0)
            elif comp_type == "tom_quantum":
                true_coords_display = state_slice_np[:, 1:3]
                obs_probs_true = np.array(
                    jax.vmap(comp_obj.process.observation_probability_distribution)(
                        jnp.asarray(state_slice_np, dtype=jnp.float32)
                    )
                )
                obs_probs_true = np.nan_to_num(obs_probs_true, nan=0.0, posinf=0.0, neginf=0.0)
                obs_sums = obs_probs_true.sum(axis=1, keepdims=True)
                valid_obs = obs_sums.squeeze(-1) > 0
                if valid_obs.any():
                    obs_probs_true[valid_obs] /= obs_sums[valid_obs, 0][:, None]
                if (~valid_obs).any():
                    obs_probs_true[~valid_obs] = 1.0 / TOM_RGB.shape[0]
                true_colors_display = np.clip(obs_probs_true @ TOM_RGB, 0.0, 1.0)
            else:
                true_coords_display = None
        except Exception as exc:  # pragma: no cover - diagnostic fallback
            print(f"    Warning: failed to resample true geometry for {comp_name} pos {pos}: {exc}")
            true_coords_display = None
            true_colors_display = None

        if true_coords_display is not None and true_coords_display.shape[0] > 4000:
            idx = choose_indices(np.arange(true_coords_display.shape[0]), 4000, seed=1)
            true_coords_display = true_coords_display[idx]
            true_colors_display = true_colors_display[idx]

        if comp_type == "mess3":
            point_size = 2.5
            panels = (
                (ax_true, true_coords_display, true_colors_display, f"{comp_name}\nTrue beliefs"),
                (ax_pred, coords_pred_s, colors_pred_s, "Predicted beliefs"),
            )
            for ax, coords, panel_colors, title in panels:
                ax.plot(tri_x, tri_y, color="black", linewidth=1.0)
                if coords is None or coords.shape[0] == 0:
                    ax.text(0.5, 0.5, "No data", ha="center", va="center", fontsize=10)
                    ax.axis("off")
                    continue
                ax.scatter(
                    coords[:, 0],
                    coords[:, 1],
                    c=panel_colors,
                    s=point_size,
                    alpha=0.5,
                    edgecolors="none",
                )
                ax.set_aspect('equal', adjustable='box')
                ax.set_xticks([])
                ax.set_yticks([])
                ax.set_title(title)

            if col == 0:
                ax_true.set_ylabel("True beliefs")
                ax_pred.set_ylabel("Predicted beliefs")

        elif comp_type == "tom_quantum":
            point_size = 2.5
            lim = 0.35
            panels = (
                (ax_true, true_coords_display, true_colors_display, f"{comp_name}\nTrue beliefs"),
                (ax_pred, coords_pred_s, colors_pred_s, "Predicted beliefs"),
            )
            for ax, coords, panel_colors, title in panels:
                if coords is None or coords.shape[0] == 0:
                    ax.text(0.5, 0.5, "No data", ha="center", va="center", fontsize=10)
                    ax.axis("off")
                    continue
                ax.scatter(
                    coords[:, 0],
                    coords[:, 1],
                    c=panel_colors,
                    s=point_size,
                    alpha=0.5,
                    edgecolors="none",
                )
                ax.set_xlim(-lim, lim)
                ax.set_ylim(-lim, lim)
                ax.set_aspect('equal', adjustable='box')
                ax.grid(True, linewidth=0.3, alpha=0.4)
                ax.set_title(title)
                if col == 0:
                    ax.set_xlabel("Re coherence")
                    ax.set_ylabel("Im coherence")
                else:
                    ax.set_xticks([])
                    ax.set_yticks([])
        else:
            for ax in (ax_true, ax_pred):
                ax.text(0.5, 0.5, f"Unsupported type\n{comp_type}", ha="center", va="center", fontsize=10)
                ax.axis('off')
            continue

    fig.suptitle(f"Belief vs predicted geometry (position {pos})", fontsize=15)
    fig.tight_layout(rect=(0, 0.02, 1, 0.95))
    os.makedirs(args.fig_out_dir, exist_ok=True)
    grid_out = os.path.join(args.fig_out_dir, f"belief_prediction_grid_pos{pos}.png")
    fig.savefig(grid_out, dpi=170)
    plt.close(fig)
    print(f"Saved composite belief/prediction grid → {grid_out}")


#%%
# ==== Ground-truth Belief Geometry Sweeps ============================================= #

os.makedirs(args.fig_out_dir, exist_ok=True)

mess3_x_grid = np.linspace(0.10, 0.22, 7)
mess3_a_grid = np.linspace(0.20, 0.80, 8)
mess3_grid_fig = plot_mess3_belief_grid(
    x_values=mess3_x_grid,
    a_values=mess3_a_grid,
    seq_position=9,
    batch_size=4096,
    seq_len=seq_len,
    seed=123,
    sample_size=2500,
)
mess3_grid_path = os.path.join(args.fig_out_dir, "mess3_belief_grid_pos9.png")
mess3_grid_fig.savefig(mess3_grid_path, dpi=170)
plt.close(mess3_grid_fig)
print(f"Saved Mess3 parameter grid → {mess3_grid_path}")

tomq_08013_fig = plot_tom_quantum_coherence(
    alpha=0.8,
    beta=1.3,
    seq_position=9,
    batch_size=4096,
    seq_len=seq_len,
    seed=432,
    sample_size=4000,
)
tomq_08013_path = os.path.join(args.fig_out_dir, "tomq_true_geometry_alpha0.8_beta1.3_pos9.png")
tomq_08013_fig.savefig(tomq_08013_path, dpi=180)
plt.close(tomq_08013_fig)
print(f"Saved TomQ true geometry (alpha=0.8, beta=1.3) → {tomq_08013_path}")

central_tomq_fig = plot_tom_quantum_coherence(
    alpha=1.0,
    beta=float(np.sqrt(51)),
    seq_position=9,
    batch_size=4096,
    seq_len=seq_len,
    seed=456,
    sample_size=4000,
)
tomq_central_path = os.path.join(args.fig_out_dir, "tomq_true_geometry_alpha1.0_beta_sqrt51_pos9.png")
central_tomq_fig.savefig(tomq_central_path, dpi=180)
plt.close(central_tomq_fig)
print(f"Saved TomQ true geometry (alpha=1.0, beta=sqrt(51)) → {tomq_central_path}")

alpha_center = 1.0
beta_center = float(np.sqrt(51))
tomq_alpha_grid = np.linspace(alpha_center - 0.12, alpha_center + 0.12, 5)
tomq_beta_grid = np.linspace(beta_center - 1.5, beta_center + 1.5, 5)
tomq_grid_sweep_fig = plot_tom_quantum_coherence_grid(
    alpha_values=tomq_alpha_grid,
    beta_values=tomq_beta_grid,
    seq_position=9,
    batch_size=4096,
    seq_len=seq_len,
    seed=789,
    sample_size=3500,
)
tomq_grid_sweep_path = os.path.join(args.fig_out_dir, "tomq_coherence_grid_pos9.png")
tomq_grid_sweep_fig.savefig(tomq_grid_sweep_path, dpi=170)
plt.close(tomq_grid_sweep_fig)
print(f"Saved TomQ parameter grid → {tomq_grid_sweep_path}")

wide_alpha_grid = np.linspace(max(alpha_center - 0.5, 0.5), alpha_center + 0.5, 9)
wide_beta_grid = np.linspace(max(beta_center - 4.0, 1.0), beta_center + 4.0, 9)
tomq_grid_wide_fig = plot_tom_quantum_coherence_grid(
    alpha_values=wide_alpha_grid,
    beta_values=wide_beta_grid,
    seq_position=9,
    batch_size=4096,
    seq_len=seq_len,
    seed=987,
    sample_size=3000,
)
tomq_grid_wide_path = os.path.join(args.fig_out_dir, "tomq_coherence_grid_wide_pos9.png")
tomq_grid_wide_fig.savefig(tomq_grid_wide_path, dpi=170)
plt.close(tomq_grid_wide_fig)
print(f"Saved TomQ wide parameter grid → {tomq_grid_wide_path}")


#%%
# ==== Plot PC Projections ============================================================= #
# ==== for All 9 choose 3 and 9 choose 2 combinations of the first 9 indices of PCA ==== #
##########################################################################################


# # Test. Don't delete
# test_inds = [1,2]
# if len(test_inds) == 2:
#     test_title = f"PCA PCs {test_inds[0]+1},{test_inds[1]+1}"
# elif len(test_inds) == 3:
#     test_title = f"3D PCA PCs {test_inds[0]+1},{test_inds[1]+1},{test_inds[2]+1}"
# else:
#     raise ValueError(f"Invalid number of indices: {len(test_inds)}")
# plot_pca_subplots(
#     pca_coords,
#     mess3_point_colors,
#     tom_point_colors,
#     pc_indices=test_inds,
#     marker_size=3,
#     opacity=0.6,
#     height=900,
#     width=900,
#     show=True,
#     title_text=test_title,
#     output_dir=args.fig_out_dir,
#     save=None,
#     mess3_label_to_color=mess3_label_to_color,
#     tom_label_to_color=tom_label_to_color,
#     n_points_to_plot=3000
# )
# n = 1


#%%
pca_indices = list(range(pca_coords.shape[1]))
if len(pca_indices) >= 3:
    comb_3 = list(combinations(pca_indices, 3))
    for pc_inds in comb_3:
        plot_pca_subplots(
            pca_coords,
            mess3_point_colors,
            tom_point_colors,
            pc_indices=list(pc_inds),
            marker_size=2,
            opacity=0.6,
            height=900,
            width=900,
            show=False,
            title_text=f"3D PCA PCs {pc_inds[0]+1},{pc_inds[1]+1},{pc_inds[2]+1}",
            output_dir=args.fig_out_dir,
            save=["html"],
            mess3_label_to_color=mess3_label_to_color,
            tom_label_to_color=tom_label_to_color
        )
#%%
pca_indices = list(range(pca_coords.shape[1]))
if len(pca_indices) >= 2:
    comb_2 = list(combinations(pca_indices, 2))
    for pc_inds in comb_2:
        plot_pca_subplots(
            pca_coords,
            mess3_point_colors,
            tom_point_colors,
            pc_indices=list(pc_inds),
            marker_size=3,
            opacity=0.6,
            height=700,
            width=700,
            show=False,
            title_text=f"2D PCA PCs {pc_inds[0]+1},{pc_inds[1]+1}",
            output_dir=args.fig_out_dir,
            save=["html"],
            mess3_label_to_color=mess3_label_to_color,
            tom_label_to_color=tom_label_to_color,
            n_points_to_plot=3000
        )
    print(f"{len(pca_indices)} choose 2 =", comb(len(pca_indices), 2))
#%%
