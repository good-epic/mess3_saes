# -*- coding: utf-8 -*-
"""Simplexity-Mess3.ipynb

Automatically generated by Colab.

nOriginal file is located at
    https://colab.research.google.com/drive/1lo54AkaiN8cIygtVQC7lUNdbRpRFKQWy
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Installation (skip if already installed)
# %pip -q install --upgrade pip wheel setuptools
# %pip -q install "einops>=0.7.0" "jaxtyping>=0.2.28" "beartype>=0.14" better_abc
# %pip -q install --no-deps "transformer-lens>=2.16.1"
# %pip -q install "git+https://github.com/Astera-org/simplexity.git@MATS_2025_app"


#%%
# Cell 2: Setup
import torch
import torch.nn.functional as F
import numpy as np
import jax
jax.config.update("jax_platform_name", "cpu")
import jax.numpy as jnp
from transformer_lens import HookedTransformer, HookedTransformerConfig
from simplexity.generative_processes.builder import build_hidden_markov_model, build_generalized_hidden_markov_model
from simplexity.generative_processes.torch_generator import generate_data_batch
import argparse, sys, os, json

from mess3_saes import train_sae_on_site, check_cuda_memory

import plotly.graph_objects as go
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression


#%%
# CLI arguments
device_default = "cuda" if torch.cuda.is_available() else "cpu"
parser = argparse.ArgumentParser(description="Train Transformer on mess3 and SAEs")

# HookedTransformerConfig parameters
parser.add_argument("--d_model", type=int, default=64)
parser.add_argument("--n_heads", type=int, default=1)
parser.add_argument("--n_layers", type=int, default=4)
parser.add_argument("--n_ctx", type=int, default=10)
parser.add_argument("--d_vocab", type=int, default=None)
parser.add_argument("--act_fn", type=str, default="relu")
parser.add_argument("--device", type=str, default=device_default)
parser.add_argument("--d_head", type=int, default=8)

# SAE hyperparameters
parser.add_argument("--dict_mul", type=int, default=4)
parser.add_argument("--k", type=int, nargs="+", default=[1, 2, 3, 4, 5, 6, 7])
parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=[0.01, 0.015, 0.02, 0.025, 0.05, 0.075] + [round(x, 3) for x in np.arange(0.1, 0.151, 0.005)])
parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=[1e-3, 0.01, 0.015, 0.02, 0.025, 0.05] + [round(x, 2) for x in np.arange(0.1, 0.651, 0.05)])
parser.add_argument("--input_unit_norm", dest="input_unit_norm", action="store_true", default=True)
parser.add_argument("--no_input_unit_norm", dest="input_unit_norm", action="store_false")
parser.add_argument("--n_batches_to_dead", type=int, default=5)
parser.add_argument("--top_k_aux", type=int, default=None)
parser.add_argument("--aux_penalty", type=float, default=1.0/32.0)
parser.add_argument("--bandwidth", type=float, default=0.001)

# SAE training loop controls
parser.add_argument("--sae_steps", type=int, default=1600)
parser.add_argument("--sae_batch_size", type=int, default=1024)
parser.add_argument("--sae_seq_len", type=int, default=None)
parser.add_argument("--seq_len", type=int, default=None, help="Sequence length used for analysis/visualization batches; defaults to n_ctx")

# Model loading
parser.add_argument("--load_model", type=str, default=None, help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")

# Parse known to be notebook-friendly
args, _ = parser.parse_known_args()

# Create mess3 process
mess3 = build_hidden_markov_model("mess3", x=0.1, a=0.7)
#mess3 = build_generalized_hidden_markov_model("tom_quantum", alpha=1, beta=np.sqrt(51))
print(f"mess3: vocab_size={mess3.vocab_size}, states={mess3.num_states}")

# Create TransformerLens model
device = args.device
cfg = HookedTransformerConfig(
    d_model=args.d_model,
    n_heads=args.n_heads,
    n_layers=args.n_layers,
    n_ctx=args.n_ctx,
    d_vocab=(args.d_vocab if args.d_vocab is not None else mess3.vocab_size),
    act_fn=args.act_fn,
    device=device,
    d_head=args.d_head,
)
model = HookedTransformer(cfg)
print(f"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}")

# Define a default sequence length for downstream analysis/visualization
seq_len = args.seq_len if args.seq_len is not None else cfg.n_ctx


#%%
## Train or load model

key = jax.random.PRNGKey(42)
args.load_model = "outputs/mess3_transformer.pt"

# Cell 3: Train (or load)
losses = []
if args.load_model is None:
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    batch_size, seq_len = 1024, cfg.n_ctx
    # Get stationary distribution for initial states
    stationary = mess3.initial_state
    # Create the tqdm progress bar object
    progress_bar = tqdm(range(25000), desc="Training")
    for step in progress_bar:
        key, subkey = jax.random.split(key)
        gen_states = jnp.repeat(stationary[None, :], batch_size, axis=0)
        _, inputs, _ = generate_data_batch(gen_states, mess3, batch_size, seq_len+1, subkey)
        if isinstance(inputs, torch.Tensor):
            tokens = inputs.long().to(device)
        else:
            tokens = torch.from_numpy(np.array(inputs)).long().to(device)
        loss = model(tokens, return_type="loss")
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        progress_bar.set_description(f"Training (Loss: {loss.item():.4f})")
    print(f"\nFinal loss: {losses[-1]:.4f} (started at {losses[0]:.4f})")

    # Quick visualization
    plt.plot(losses)
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.show()

    ## Save trained model
    os.makedirs("outputs", exist_ok=True)
    save_path = os.path.join("outputs", "mess3_transformer.pt")
    checkpoint = {
        "state_dict": model.state_dict(),
        "config": cfg.to_dict() if hasattr(cfg, "to_dict") else vars(cfg),
    }
    torch.save(checkpoint, save_path)
    print(f"Saved trained model to {save_path}")
else:
    ckpt = torch.load(args.load_model, map_location=device, weights_only=False)
    cfg_loaded = HookedTransformerConfig.from_dict(ckpt["config"]) if isinstance(ckpt.get("config"), dict) else cfg
    if isinstance(ckpt.get("config"), dict):
        model = HookedTransformer(cfg_loaded).to(device)
    model.load_state_dict(ckpt["state_dict"])  # type: ignore
    model.eval()
    print(f"Loaded model from {args.load_model}")


#%% 
## Train SAEs

# Train SAEs at the key residual stream points used below
site_to_hook = {
    "embeddings": "hook_embed",
    "layer_0": "blocks.0.hook_resid_post",
    "layer_1": "blocks.1.hook_resid_post",
    "layer_2": "blocks.2.hook_resid_post",
    "layer_3": "blocks.3.hook_resid_post",
}

sequence_saes = {}
true_coords_saes = {}
metrics_summary = {}
for site_name, hook_name in site_to_hook.items():
    print(f"Training SAEs for {site_name} at {hook_name}")
    sequence_saes[site_name], true_coords_saes[site_name], metrics_summary[site_name] = train_sae_on_site(
        site_name,
        hook_name,
        model=model,
        mess3=mess3,
        cfg=cfg,
        device=device,
        rng_key=key,
        steps=args.sae_steps,
        batch_size=args.sae_batch_size,
        seq_len=(args.sae_seq_len if args.sae_seq_len is not None else cfg.n_ctx - 1),
        k_values=args.k,
        lambda_values_seq=(args.l1_coeff_seq if args.l1_coeff_seq is not None else None),
        lambda_values_beliefs=(args.l1_coeff_beliefs if args.l1_coeff_beliefs is not None else None),
        dict_mul=args.dict_mul,
        input_unit_norm=args.input_unit_norm,
        n_batches_to_dead=args.n_batches_to_dead,
        top_k_aux=args.top_k_aux,
        aux_penalty=args.aux_penalty,
        bandwidth=args.bandwidth,
    )

#%%

# Save the entire metrics summary once across all layers
os.makedirs("outputs/saes", exist_ok=True)
metrics_path = os.path.join("outputs/saes", "metrics_summary.json")
with open(metrics_path, "w") as f:
    json.dump(metrics_summary, f, indent=2)
print(f"Saved SAE metrics summary to {metrics_path}")

#%%


