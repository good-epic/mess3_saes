# -*- coding: utf-8 -*-
"""Simplexity-Mess3.ipynb

Automatically generated by Colab.

nOriginal file is located at
    https://colab.research.google.com/drive/1lo54AkaiN8cIygtVQC7lUNdbRpRFKQWy
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Installation (skip if already installed)
# %pip -q install --upgrade pip wheel setuptools
# %pip -q install "einops>=0.7.0" "jaxtyping>=0.2.28" "beartype>=0.14" better_abc
# %pip -q install --no-deps "transformer-lens>=2.16.1"
# %pip -q install "git+https://github.com/Astera-org/simplexity.git@MATS_2025_app"


#%%
# Cell 2: Setup
import torch
import torch.nn.functional as F
import numpy as np
import jax
jax.config.update("jax_platform_name", "cpu")
import jax.numpy as jnp
from transformer_lens import HookedTransformer, HookedTransformerConfig
from simplexity.generative_processes.torch_generator import generate_data_batch
import argparse, sys, os, json
from copy import deepcopy

from mess3_saes import train_sae_on_site, train_saes_for_sites, check_cuda_memory
from multipartite_generation import (
    build_components_from_config,
    MultipartiteSampler,
)

import plotly.graph_objects as go
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression

PRESET_PROCESS_CONFIGS = {
    "single_mess3": [
        {"type": "mess3", "params": {"x": 0.1, "a": 0.7}},
    ],
    "3xmess3_2xtquant": [
        {
            "type": "mess3",
            "instances": [
                {"x": 0.12, "a": 0.55},
                {"x": 0.14, "a": 0.7},
                {"x": 0.18, "a": 0.75},
            ],
        },
        {
            "type": "tom_quantum",
            "instances": [
                {"alpha": 1.12, "beta": float(5.64)},
                {"alpha": 0.88, "beta": float(8.64)},
            ],
        },
    ],
}



#%%
# CLI arguments
device_default = "cuda" if torch.cuda.is_available() else "cpu"
parser = argparse.ArgumentParser(description="Train Transformer on mess3 and SAEs")

# HookedTransformerConfig parameters
parser.add_argument("--d_model", type=int, default=128)
parser.add_argument("--n_heads", type=int, default=4)
parser.add_argument("--n_layers", type=int, default=3)
parser.add_argument("--n_ctx", type=int, default=16)
parser.add_argument("--d_vocab", type=int, default=None)
parser.add_argument("--act_fn", type=str, default="relu")
parser.add_argument("--device", type=str, default=device_default)
parser.add_argument("--d_head", type=int, default=32)

# SAE hyperparameters
parser.add_argument("--dict_mul", type=int, default=4)
parser.add_argument("--k", type=int, nargs="+", default=list(range(1, 25)))
#parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=[0.01, 0.015, 0.02, 0.025, 0.05, 0.075] + [round(x, 3) for x in np.arange(0.1, 0.151, 0.005)])
#parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=[1e-3, 0.01, 0.015, 0.02, 0.025, 0.05] + [round(x, 2) for x in np.arange(0.1, 0.651, 0.05)])
parser.add_argument("--l1_coeff_seq", type=float, nargs="+", default=None)
parser.add_argument("--l1_coeff_beliefs", type=float, nargs="+", default=None)
parser.add_argument("--input_unit_norm", dest="input_unit_norm", action="store_true", default=True)
parser.add_argument("--no_input_unit_norm", dest="input_unit_norm", action="store_false")
parser.add_argument("--n_batches_to_dead", type=int, default=5)
parser.add_argument("--top_k_aux", type=int, default=None)
parser.add_argument("--aux_penalty", type=float, default=1.0/32.0)
parser.add_argument("--bandwidth", type=float, default=0.001)

# SAE training loop controls
parser.add_argument("--sae_steps", type=int, default=10000)
parser.add_argument("--sae_batch_size", type=int, default=1024)
parser.add_argument("--sae_seq_len", type=int, default=None)
parser.add_argument("--seq_len", type=int, default=None, help="Sequence length used for analysis/visualization batches; defaults to n_ctx")
parser.add_argument("--sae_output_dir", type=str, default="outputs/saes/multipartite_001", help="Directory to save trained SAEs and metrics")

# Model loading
#parser.add_argument("--load_model", type=str, default=None, help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--load_model", type=str, default="outputs/checkpoints/multipartite_001/checkpoint_step_500000_final.pt", help="Path to a saved model checkpoint (.pt). If provided, skip training and load this model.")
parser.add_argument("--process_config", type=str, default=None, help="Path to JSON describing a stack of generative processes")
#parser.add_argument("--process_preset", type=str, default=None, help="Named preset for generative process configuration")
parser.add_argument("--process_preset", type=str, default="3xmess3_2xtquant", help="Named preset for generative process configuration")

# Parse known to be notebook-friendly
args, _ = parser.parse_known_args()

if args.process_config and args.process_preset:
    parser.error("Specify at most one of --process_config or --process_preset")

if args.process_config:
    with open(args.process_config, "r", encoding="utf-8") as f:
        process_config = json.load(f)
elif args.process_preset:
    if args.process_preset not in PRESET_PROCESS_CONFIGS:
        parser.error(f"Unknown process preset '{args.process_preset}'")
    process_config = deepcopy(PRESET_PROCESS_CONFIGS[args.process_preset])
else:
    process_config = deepcopy(PRESET_PROCESS_CONFIGS["single_mess3"])

components = build_components_from_config(process_config)
if len(components) == 1:
    data_source = components[0].process
    sampler: MultipartiteSampler | None = None
else:
    sampler = MultipartiteSampler(components)
    data_source = sampler

if isinstance(data_source, MultipartiteSampler):
    vocab_size = data_source.vocab_size
    num_states = data_source.belief_dim
    print(
        f"Multipartite process with components {[c.name for c in data_source.components]}"
        f" â†’ vocab_size={vocab_size}, belief_dim={num_states}"
    )
else:
    vocab_size = data_source.vocab_size
    num_states = data_source.num_states
    print(f"Process: vocab_size={vocab_size}, states={num_states}")

# Create TransformerLens model
device = args.device
cfg = HookedTransformerConfig(
    d_model=args.d_model,
    n_heads=args.n_heads,
    n_layers=args.n_layers,
    n_ctx=args.n_ctx,
    d_vocab=(args.d_vocab if args.d_vocab is not None else vocab_size),
    act_fn=args.act_fn,
    device=device,
    d_head=args.d_head,
)
model = HookedTransformer(cfg)
print(f"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}")

# Define a default sequence length for downstream analysis/visualization
seq_len = args.seq_len if args.seq_len is not None else cfg.n_ctx


#%%
## Train or load model

key = jax.random.PRNGKey(42)
# Cell 3: Train (or load)
losses = []
if args.load_model is None:
    if isinstance(data_source, MultipartiteSampler):
        raise ValueError(
            "Training from scratch is not implemented for multipartite stacks; "
            "please provide --load_model pointing to a pretrained checkpoint."
        )
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    batch_size, seq_len = 1024, cfg.n_ctx
    # Get stationary distribution for initial states
    stationary = data_source.initial_state
    # Create the tqdm progress bar object
    progress_bar = tqdm(range(25000), desc="Training", miniters=1_000, disable=not sys.stderr.isatty())
    for step in progress_bar:
        key, subkey = jax.random.split(key)
        gen_states = jnp.repeat(stationary[None, :], batch_size, axis=0)
        _, inputs, _ = generate_data_batch(gen_states, data_source, batch_size, seq_len+1, subkey)
        if isinstance(inputs, torch.Tensor):
            tokens = inputs.long().to(device)
        else:
            tokens = torch.from_numpy(np.array(inputs)).long().to(device)
        loss = model(tokens, return_type="loss")
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        progress_bar.set_description(f"Training (Loss: {loss.item():.4f})", refresh=False)
    print(f"\nFinal loss: {losses[-1]:.4f} (started at {losses[0]:.4f})")

    # Quick visualization
    plt.plot(losses)
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.show()

    ## Save trained model
    os.makedirs("outputs", exist_ok=True)
    save_path = os.path.join("outputs", "mess3_transformer.pt")
    checkpoint = {
        "state_dict": model.state_dict(),
        "config": cfg.to_dict() if hasattr(cfg, "to_dict") else vars(cfg),
    }
    torch.save(checkpoint, save_path)
    print(f"Saved trained model to {save_path}")
else:
    ckpt = torch.load(args.load_model, map_location=device, weights_only=False)
    cfg_loaded = None
    if isinstance(ckpt.get("config"), dict):
        cfg_loaded = HookedTransformerConfig.from_dict(ckpt["config"])
        model = HookedTransformer(cfg_loaded).to(device)
    state_dict = ckpt.get("state_dict") or ckpt.get("model_state_dict")
    if state_dict is None:
        raise KeyError("Checkpoint missing Transformer state dict")
    if cfg_loaded is None:
        model = HookedTransformer(cfg).to(device)
    model.load_state_dict(state_dict)  # type: ignore[arg-type]
    model.eval()
    print(f"Loaded model from {args.load_model}")


#%% 
## Train SAEs

# Train SAEs at the key residual stream points used below
site_to_hook = {"embeddings": "hook_embed"}
for layer_idx in range(cfg.n_layers):
    site_to_hook[f"layer_{layer_idx}"] = f"blocks.{layer_idx}.hook_resid_post"

sequence_saes = {}
true_coords_saes = {}
metrics_summary = {}
print("Training SAEs for all sites in one pass")
sequence_saes, true_coords_saes, metrics_summary = train_saes_for_sites(
    site_to_hook,
    model=model,
    data_source=data_source,
    cfg=cfg,
    device=device,
    rng_key=key,
    steps=args.sae_steps,
    batch_size=args.sae_batch_size,
    seq_len=(args.sae_seq_len if args.sae_seq_len is not None else cfg.n_ctx - 1),
    k_values=args.k,
    lambda_values_seq=(args.l1_coeff_seq if args.l1_coeff_seq is not None else None),
    lambda_values_beliefs=(args.l1_coeff_beliefs if args.l1_coeff_beliefs is not None else None),
    dict_mul=args.dict_mul,
    input_unit_norm=args.input_unit_norm,
    n_batches_to_dead=args.n_batches_to_dead,
    top_k_aux=args.top_k_aux,
    aux_penalty=args.aux_penalty,
    bandwidth=args.bandwidth,
    belief_dim=num_states,
    sae_output_dir=args.sae_output_dir,
)

#%%

# Save the entire metrics summary once across all layers
os.makedirs(args.sae_output_dir, exist_ok=True)
metrics_path = os.path.join(args.sae_output_dir, "metrics_summary.json")
with open(metrics_path, "w") as f:
    json.dump(metrics_summary, f, indent=2)
print(f"Saved SAE metrics summary to {metrics_path}")

#%%
